{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xb1aLHZRFrwA"
   },
   "source": [
    "# **Ludwig + DeepLearning.ai: Efficient Fine-Tuning for Llama2-7b on a Single GPU** 🙌\n",
    "\n",
    "Let's explore how to fine-tune an LLM on a single commodity GPU with [Ludwig](https://ludwig.ai/latest/), an open-source package that empowers you to effortlessly build and train machine learning models like LLMs, neural networks and tree based models through declarative config files.\n",
    "\n",
    "In this notebook, we'll show an example of how to fine-tune Llama-2-7b to generate code using the CodeAlpaca dataset.\n",
    "\n",
    "By the end of this example, you will have gained a comprehensive understanding of the following key aspects:\n",
    "\n",
    "1. **Ludwig**: An intuitive toolkit that simplifies fine-tuning for open-source Language Model Models (LLMs).\n",
    "2. **Exploring the base model with prompts**: Dive into the intricacies of prompts and prompt templates, unlocking new dimensions in LLM interaction.\n",
    "3. **Fine-Tuning Large Language Models**: Navigate the world of model fine-tuning optimizations for getting the most out of a single memory-contrained GPU, including: LoRA and 4-bit quantization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXMVoEFkPXJJ"
   },
   "source": [
    "### **Install Ludwig and Ludwig's LLM related dependencies.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiZYaiRufHfh"
   },
   "source": [
    "Install Ludwig from the latest release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jvL1cL6wYtWz"
   },
   "outputs": [],
   "source": [
    "# !pip uninstall -y tensorflow --quiet\n",
    "# !pip install ludwig\n",
    "# !pip install ludwig[llm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7eAUs6efGH5"
   },
   "source": [
    "Install Ludwig from Ludwig master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yfYXyUUvM-R6",
    "outputId": "74e2cb69-f04d-485f-ef18-2db6c70619be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mDEPRECATION: git+https://github.com/ludwig-ai/ludwig.git@master#egg=ludwig[llm] contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y tensorflow --quiet\n",
    "!pip install git+https://github.com/ludwig-ai/ludwig.git@master --quiet\n",
    "!pip install \"git+https://github.com/ludwig-ai/ludwig.git@master#egg=ludwig[llm]\" --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2SRHcKAJYuu"
   },
   "source": [
    "Enable text wrapping so we don't have to scroll horizontally and create a function to flush CUDA cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ht4eVWxB13QL"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "\n",
    "get_ipython().events.register('pre_run_cell', set_css)\n",
    "\n",
    "def clear_cache():\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUkPQ9OLP0Po"
   },
   "source": [
    "### **Setup Your HuggingFace Token** 🤗\n",
    "\n",
    "We'll be exploring Llama-2 today, which a model released by Meta. However, the model is not openly-accessible and requires requesting for access (assigned to your HuggingFace token).\n",
    "\n",
    "Obtain a [HuggingFace API Token](https://huggingface.co/settings/tokens) and request access to [Llama2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf) before proceeding. You may need to signup on HuggingFace if you don't aleady have an account: https://huggingface.co/join\n",
    "\n",
    "Incase you haven't been given access to Llama-2-7b, that is alright. We can just use Llama-1 for the rest of this example: [huggyllama/llama-7b](https://huggingface.co/huggyllama/llama-7b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "P0AGbGklFKo3",
    "outputId": "a48dab25-d1f5-4587-fbfe-801e5d4b640a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f865bb5f6d0> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f865bbf26e0, raw_cell=\"# !pip uninstall torch -y\n",
      "# !pip install torch\n",
      "# i..\" store_history=True silent=False shell_futures=True cell_id=f8c8f927-991b-4fa4-8caa-4a995f113a26>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "# !pip uninstall torch -y\n",
    "# !pip install torch\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "GA89vvxMHzrF",
    "outputId": "b580a8ce-f3a8-4535-af94-ef044eb2e1a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f865bb5f6d0> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f865827c6d0, raw_cell=\"# # !pip uninstall torch torchtext -y\n",
      "# !pip insta..\" store_history=True silent=False shell_futures=True cell_id=39b95633-6624-47cc-99a1-9073be6f9209>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "# # !pip uninstall torch torchtext -y\n",
    "# !pip install torch torchtext\n",
    "# # !pip uninstall torchaudio -y\n",
    "# !pip install torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "nXOIxmfbQHSz",
    "outputId": "d2d43f0f-d32b-48c5-9404-a421af5bfac3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f7f836a3010> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f7f83712c80, raw_cell=\"import locale\n",
      "\n",
      "# Override getpreferredencoding to ..\" store_history=True silent=False shell_futures=True cell_id=933b638d-a7f1-4444-b785-934dfcb762f1>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.1.0+cu118 available.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Token: ········\n"
     ]
    }
   ],
   "source": [
    "import locale\n",
    "\n",
    "# Override getpreferredencoding to always return UTF-8\n",
    "locale.getpreferredencoding = lambda _=None: \"UTF-8\"\n",
    "\n",
    "import getpass\n",
    "# import locale; locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "import logging\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "from ludwig.api import LudwigModel\n",
    "\n",
    "\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = getpass.getpass(\"Token:\")\n",
    "assert os.environ[\"HUGGING_FACE_HUB_TOKEN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sV0D2rjRbk4z"
   },
   "source": [
    "### **Import The Code Generation Dataset** 📋\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "cAsqyCDX-_NA",
    "outputId": "5abeeef7-11a4-4b47-e3e6-80c11c5f37a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f865bb5f6d0> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f841e054520, raw_cell=\"# from google.colab import drive\n",
      "# drive.mount('/c..\" store_history=True silent=False shell_futures=True cell_id=05e8c065-cf53-4fcb-96cf-3e6738504009>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "m8FXsCwYbkHh",
    "outputId": "dd4a0756-af11-4c3e-b1be-e95e775a927f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f865bb5f6d0> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f841e054400, raw_cell=\"# from google.colab import data_table; data_table...\" store_history=True silent=False shell_futures=True cell_id=e8e6dd4b-ab0e-487f-a5a6-8fd5cbe80ee9>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "# from google.colab import data_table; data_table.enable_dataframe_formatter()\n",
    "# import numpy as np; np.random.seed(123)\n",
    "# import pandas as pd\n",
    "\n",
    "# import json\n",
    "# with open('/content/drive/MyDrive/arxiv_physics_instruct_30k.jsonl') as f1:\n",
    "#     data1 = [json.loads(line) for line in f1]\n",
    "# with open('/content/drive/MyDrive/arxiv_math_instruct_50k.jsonl') as f2:\n",
    "#     data2 = [json.loads(line) for line in f2]\n",
    "\n",
    "# df1 = pd.DataFrame(data1)\n",
    "# df2 = pd.DataFrame(data2)\n",
    "# df = pd.concat([df1, df2])\n",
    "# main_df = df.sample(frac=1, random_state=42)\n",
    "# main_df.reset_index(drop=True, inplace=True)\n",
    "# # We're going to create a new column called `split` where:\n",
    "# # 90% will be assigned a value of 0 -> train set\n",
    "# # 5% will be assigned a value of 1 -> validation set\n",
    "# # 5% will be assigned a value of 2 -> test set\n",
    "# # Calculate the number of rows for each split value\n",
    "# total_rows = len(main_df)\n",
    "# split_0_count = int(total_rows * 0.9)\n",
    "# split_1_count = int(total_rows * 0.05)\n",
    "# split_2_count = total_rows - split_0_count - split_1_count\n",
    "\n",
    "# # Create an array with split values based on the counts\n",
    "# split_values = np.concatenate([\n",
    "#     np.zeros(split_0_count),\n",
    "#     np.ones(split_1_count),\n",
    "#     np.full(split_2_count, 2)\n",
    "# ])\n",
    "\n",
    "# # Shuffle the array to ensure randomness\n",
    "# np.random.shuffle(split_values)\n",
    "\n",
    "# # Add the 'split' column to the DataFrame\n",
    "# main_df['split'] = split_values\n",
    "# main_df['split'] = main_df['split'].astype(int)\n",
    "\n",
    "# # For this webinar, we will just 500 rows of this dataset.\n",
    "# main_df = main_df.head(n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2253
    },
    "id": "N-UW-zeKLsjZ",
    "outputId": "6760198b-04ad-4d1b-8631-d5f49293d584"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f865bb5f6d0> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f841e054940, raw_cell=\"# from google.colab import data_table; data_table...\" store_history=True silent=False shell_futures=True cell_id=d344b8b8-5fb9-4778-ac3e-0ee0506f2851>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "# from google.colab import data_table; data_table.enable_dataframe_formatter()\n",
    "# import numpy as np; np.random.seed(123)\n",
    "# import pandas as pd\n",
    "# import json\n",
    "\n",
    "# # Enable DataFrame formatter\n",
    "# data_table.enable_dataframe_formatter()\n",
    "\n",
    "# # Load data from the JSONL file\n",
    "# jsonl_file_path = 'train_data.jsonl'\n",
    "# with open(jsonl_file_path) as f:\n",
    "#     data = [json.loads(line) for line in f]\n",
    "\n",
    "# # Create DataFrame from the loaded data\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Shuffle the DataFrame\n",
    "# main_df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# # We're going to create a new column called `split` where:\n",
    "# # 90% will be assigned a value of 0 -> train set\n",
    "# # 5% will be assigned a value of 1 -> validation set\n",
    "# # 5% will be assigned a value of 2 -> test set\n",
    "\n",
    "# # Calculate the number of rows for each split value\n",
    "# total_rows = len(main_df)\n",
    "# split_0_count = int(total_rows * 0.9)\n",
    "# split_1_count = int(total_rows * 0.05)\n",
    "# split_2_count = total_rows - split_0_count - split_1_count\n",
    "\n",
    "# # Create an array with split values based on the counts\n",
    "# split_values = np.concatenate([\n",
    "#     np.zeros(split_0_count),\n",
    "#     np.ones(split_1_count),\n",
    "#     np.full(split_2_count, 2)\n",
    "# ])\n",
    "\n",
    "# # Shuffle the array to ensure randomness\n",
    "# np.random.shuffle(split_values)\n",
    "\n",
    "# # Add the 'split' column to the DataFrame\n",
    "# main_df['split'] = split_values.astype(int)\n",
    "\n",
    "# # Display a sample of the DataFrame\n",
    "# main_df.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f7f836a3010> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f7d51c7cc40, raw_cell=\"import numpy as np\n",
      "import pandas as pd\n",
      "import json..\" store_history=True silent=False shell_futures=True cell_id=314a4da5-1fb5-4abd-b138-c4c0b7158410>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>instruction</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1558</th>\n",
       "      <td>\\n\\nTitle: Watercress: A Rich Source of Bioact...</td>\n",
       "      <td>[[3,4-dihydroxy-5-all-trans-hexaprenylbenzoate...</td>\n",
       "      <td>The task is to extract chemical-related triple...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>\\n\\nTitle: Chemical Constituents of Angelica: ...</td>\n",
       "      <td>[[Ureidoisobutyric acid, sourced through, Ange...</td>\n",
       "      <td>The task is to extract chemical-related triple...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>\\n\\nTitle: Biolocation of Pharmaceutical Compo...</td>\n",
       "      <td>[[suloctidil, biolocation is, Blood], [Polymyx...</td>\n",
       "      <td>The task is to extract chemical-related triple...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>\\n\\nTitle: Sourcing of Bioactive Compounds in ...</td>\n",
       "      <td>[[5-Methylthioribose, sourced through, Robusta...</td>\n",
       "      <td>The task is to extract chemical-related triple...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072</th>\n",
       "      <td>\\n\\nTitle: The Role of PA(16:0/18:1(11Z)) in C...</td>\n",
       "      <td>[[PA(16:0/18:1(11Z)), involved in, Cardiolipin...</td>\n",
       "      <td>The task is to extract chemical-related triple...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  input  \\\n",
       "1558  \\n\\nTitle: Watercress: A Rich Source of Bioact...   \n",
       "521   \\n\\nTitle: Chemical Constituents of Angelica: ...   \n",
       "1005  \\n\\nTitle: Biolocation of Pharmaceutical Compo...   \n",
       "1545  \\n\\nTitle: Sourcing of Bioactive Compounds in ...   \n",
       "1072  \\n\\nTitle: The Role of PA(16:0/18:1(11Z)) in C...   \n",
       "\n",
       "                                                 output  \\\n",
       "1558  [[3,4-dihydroxy-5-all-trans-hexaprenylbenzoate...   \n",
       "521   [[Ureidoisobutyric acid, sourced through, Ange...   \n",
       "1005  [[suloctidil, biolocation is, Blood], [Polymyx...   \n",
       "1545  [[5-Methylthioribose, sourced through, Robusta...   \n",
       "1072  [[PA(16:0/18:1(11Z)), involved in, Cardiolipin...   \n",
       "\n",
       "                                            instruction  split  \n",
       "1558  The task is to extract chemical-related triple...      0  \n",
       "521   The task is to extract chemical-related triple...      0  \n",
       "1005  The task is to extract chemical-related triple...      0  \n",
       "1545  The task is to extract chemical-related triple...      0  \n",
       "1072  The task is to extract chemical-related triple...      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load data from the JSONL file\n",
    "jsonl_file_path = 'ludwig_train_data.jsonl'\n",
    "with open(jsonl_file_path) as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# Create DataFrame from the loaded data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "main_df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# We're going to create a new column called `split` where:\n",
    "# 90% will be assigned a value of 0 -> train set\n",
    "# 5% will be assigned a value of 1 -> validation set\n",
    "# 5% will be assigned a value of 2 -> test set\n",
    "\n",
    "# Calculate the number of rows for each split value\n",
    "total_rows = len(main_df)\n",
    "split_0_count = int(total_rows * 0.9)\n",
    "split_1_count = int(total_rows * 0.05)\n",
    "split_2_count = total_rows - split_0_count - split_1_count\n",
    "\n",
    "# Create an array with split values based on the counts\n",
    "split_values = np.concatenate([\n",
    "    np.zeros(split_0_count),\n",
    "    np.ones(split_1_count),\n",
    "    np.full(split_2_count, 2)\n",
    "])\n",
    "\n",
    "# Shuffle the array to ensure randomness\n",
    "np.random.shuffle(split_values)\n",
    "\n",
    "# Add the 'split' column to the DataFrame\n",
    "main_df['split'] = split_values.astype(int)\n",
    "\n",
    "# Display a sample of the DataFrame\n",
    "main_df.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "bvfE3EZ677WT",
    "outputId": "6bf4e277-0961-4176-ae55-74a0f230c183"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f7f836a3010> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f7d51c7ecb0, raw_cell=\"len(main_df)\" store_history=True silent=False shell_futures=True cell_id=55f9b6b1-a72c-4320-9fcf-687cc732b1a6>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1879"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4211
    },
    "id": "Mx3JQMpmb1KQ",
    "outputId": "27472829-9f18-4007-9119-f8eaf79a6dc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f7f836a3010> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f7f837129e0, raw_cell=\"main_df.head(n=10)\" store_history=True silent=False shell_futures=True cell_id=83aa2908-3d20-478e-ab0c-193eccca313c>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>instruction</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nCytidine triphosphate (CTP) is a crucial m...</td>\n",
       "      <td>[[Cytidine triphosphate, involved in, Cardioli...</td>\n",
       "      <td>The task is to extract chemical-related triple...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nTitle: LysoPA(18:2(9Z,12Z)/0:0): A Key Pla...</td>\n",
       "      <td>[[LysoPA(18:2(9Z,12Z)/0:0), involved in, Cardi...</td>\n",
       "      <td>The task is to extract chemical-related triple...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nTitle: The Role of Neotussilagolactone, Et...</td>\n",
       "      <td>[[Neotussilagolactone, has role of, Surfactant...</td>\n",
       "      <td>The task is to extract chemical-related triple...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nTitle: Sourcing L-Canaline and Related Com...</td>\n",
       "      <td>[[L-canaline, sourced through, Daikon radish],...</td>\n",
       "      <td>The task is to extract chemical-related triple...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\nTitle: Phosphatidic Acid (PA) (16:0/18:1(1...</td>\n",
       "      <td>[[PA(16:0/18:1(11Z)), involved in, De Novo Tri...</td>\n",
       "      <td>The task is to extract chemical-related triple...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n\\nTitle: LysoPA(16:0/0:0) Involvement in Lip...</td>\n",
       "      <td>[[LysoPA(16:0/0:0), involved in, Cardiolipin B...</td>\n",
       "      <td>The task is to extract chemical-related triple...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\n\\nTitle: Roles of Phosphate in De Novo Triac...</td>\n",
       "      <td>[[Phosphate, involved in, De Novo Triacylglyce...</td>\n",
       "      <td>The task is to extract chemical-related triple...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n\\nTitle: Sources of Cis-Zeatin-9-N-Glucoside...</td>\n",
       "      <td>[[cis-Zeatin-9-N-glucoside, sourced through, I...</td>\n",
       "      <td>The task is to extract chemical-related triple...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\n\\nTitle: Palmityl-CoA: A Key Player in Lipid...</td>\n",
       "      <td>[[Palmityl-CoA, involved in, De Novo Triacylgl...</td>\n",
       "      <td>The task is to extract chemical-related triple...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\n\\nTitle: Exploring the Phytochemical Composi...</td>\n",
       "      <td>[[trans-p-Coumaric acid, sourced through, Bitt...</td>\n",
       "      <td>The task is to extract chemical-related triple...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  \\n\\nCytidine triphosphate (CTP) is a crucial m...   \n",
       "1  \\n\\nTitle: LysoPA(18:2(9Z,12Z)/0:0): A Key Pla...   \n",
       "2  \\n\\nTitle: The Role of Neotussilagolactone, Et...   \n",
       "3  \\n\\nTitle: Sourcing L-Canaline and Related Com...   \n",
       "4  \\n\\nTitle: Phosphatidic Acid (PA) (16:0/18:1(1...   \n",
       "5  \\n\\nTitle: LysoPA(16:0/0:0) Involvement in Lip...   \n",
       "6  \\n\\nTitle: Roles of Phosphate in De Novo Triac...   \n",
       "7  \\n\\nTitle: Sources of Cis-Zeatin-9-N-Glucoside...   \n",
       "8  \\n\\nTitle: Palmityl-CoA: A Key Player in Lipid...   \n",
       "9  \\n\\nTitle: Exploring the Phytochemical Composi...   \n",
       "\n",
       "                                              output  \\\n",
       "0  [[Cytidine triphosphate, involved in, Cardioli...   \n",
       "1  [[LysoPA(18:2(9Z,12Z)/0:0), involved in, Cardi...   \n",
       "2  [[Neotussilagolactone, has role of, Surfactant...   \n",
       "3  [[L-canaline, sourced through, Daikon radish],...   \n",
       "4  [[PA(16:0/18:1(11Z)), involved in, De Novo Tri...   \n",
       "5  [[LysoPA(16:0/0:0), involved in, Cardiolipin B...   \n",
       "6  [[Phosphate, involved in, De Novo Triacylglyce...   \n",
       "7  [[cis-Zeatin-9-N-glucoside, sourced through, I...   \n",
       "8  [[Palmityl-CoA, involved in, De Novo Triacylgl...   \n",
       "9  [[trans-p-Coumaric acid, sourced through, Bitt...   \n",
       "\n",
       "                                         instruction  split  \n",
       "0  The task is to extract chemical-related triple...      2  \n",
       "1  The task is to extract chemical-related triple...      0  \n",
       "2  The task is to extract chemical-related triple...      0  \n",
       "3  The task is to extract chemical-related triple...      0  \n",
       "4  The task is to extract chemical-related triple...      0  \n",
       "5  The task is to extract chemical-related triple...      0  \n",
       "6  The task is to extract chemical-related triple...      0  \n",
       "7  The task is to extract chemical-related triple...      0  \n",
       "8  The task is to extract chemical-related triple...      0  \n",
       "9  The task is to extract chemical-related triple...      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eIyMPoSGFAN"
   },
   "source": [
    "As you can see below, the dataset is pretty balanced in terms of the number of examples of each type of instruction (also true for the full dataset with 20,000 rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "REqFr-ngx0et",
    "outputId": "7158cf56-3205-4fa1-a280-af6c96b6a0c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f6143fb3490> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f5eec7030a0, raw_cell=\"# num_self_sufficient = (df['input'] == '').sum()\n",
      "..\" store_history=True silent=False shell_futures=True cell_id=a2933d48-6daf-4f03-b4e5-3f3cd2f39ed9>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples in the dataset: 1879\n",
      "% of examples that are need additional context: 100.0\n"
     ]
    }
   ],
   "source": [
    "# num_self_sufficient = (df['input'] == '').sum()\n",
    "num_need_context = main_df.shape[0]\n",
    "# print(num_need_context)\n",
    "# We are only using 100 rows of this dataset for this webinar\n",
    "print(f\"Total number of examples in the dataset: {main_df.shape[0]}\")\n",
    "\n",
    "# print(f\"% of examples that are self-sufficient: {round(num_self_sufficient/main_df.shape[0] * 100, 2)}\")\n",
    "print(f\"% of examples that are need additional context: {round(num_need_context/main_df.shape[0] * 100, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcM8DVrIKXWj"
   },
   "source": [
    "The other aspect worth noting is the average number of characters in each of the three columns `instruction`, `input` and `output` in the dataset. Typically, every 3-4 characters maps to a *token* (the basic building blocks that language models use to understand and analyze text data), and large language models have a limit on the number of tokens they can take as input.\n",
    "\n",
    "The maximum context length for the base LLaMA-2 model is 4096 tokens. Ludwig automatically truncates texts that are too long for the model, but looking at these sequence lengths, we should be able to fine-tune on full length examples without needing any truncation.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "_3x3stdDKoCT",
    "outputId": "6da3dabc-1200-4851-f5c0-416b19b3e5f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f865bb5f6d0> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f841e0572e0, raw_cell=\"# # Calculating the length of each cell in each co..\" store_history=True silent=False shell_futures=True cell_id=ce2b9fd3-072d-456a-b50e-4413460d69eb>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "# # Calculating the length of each cell in each column\n",
    "# df['num_characters_question'] = df['question'].apply(lambda x: len(x))\n",
    "# # df['num_characters_input'] = df['input'].apply(lambda x: len(x))\n",
    "# df['num_characters_answer'] = df['answer'].apply(lambda x: len(x))\n",
    "\n",
    "# # Show Distribution\n",
    "# df.hist(column=['num_characters_question', 'num_characters_answer'])\n",
    "\n",
    "# # Calculating the average\n",
    "# average_chars_instruction = df['num_characters_question'].mean()\n",
    "# # average_chars_input = df['num_characters_input'].mean()\n",
    "# average_chars_output = df['num_characters_answer'].mean()\n",
    "\n",
    "# print(f'Average number of tokens in the instruction column: {(average_chars_instruction / 3):.0f}')\n",
    "# # print(f'Average number of tokens in the input column: {(average_chars_input / 3):.0f}')\n",
    "# print(f'Average number of tokens in the output column: {(average_chars_output / 3):.0f}', end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oU8iKtqwCc1b"
   },
   "source": [
    "There are three different fine-tuning approaches in Ludwig:\n",
    "\n",
    "1. **Full Fine-Tuning**:\n",
    "\n",
    "- Involves training the entire pre-trained model on new data from scratch.\n",
    "- All model layers and parameters are updated during fine-tuning.\n",
    "- Can lead to high accuracy but requires a significant amount of computational resources and time.\n",
    "- Runs the risk of catastrophic forgetting: occasionally, since we are updating all of the weights in the model, this process can lead to the algorithm inadvertently losing knowledge of its past tasks, i.e., the knowledge it gained during pretraining. The outcome may vary, with the algorithm experiencing heightened error margins in some cases, while in others, it might completely erase the memory of a specific task leading to terrible performance.\n",
    "- Best suited when the target task is significantly different from the original pre-training task.\n",
    "\n",
    "2. **Parameter Efficient Fine-Tuning (PEFT), e.g. LoRA**:\n",
    "\n",
    "- Focuses on updating only a subset of the model's parameters.\n",
    "- Often involves freezing certain layers or parts of the model to avoid catastrophic forgetting, or inserting additional layers that are trainable while keeping the original model's weights frozen.\n",
    "- Can result in faster fine-tuning with fewer computational resources, but might sacrifice some accuracy compared to full fine-tuning.\n",
    "- Includes methods like LoRA, AdaLoRA and Adaption Prompt (LLaMA Adapter)\n",
    "- Suitable when the new task shares similarities with the original pre-training task.\n",
    "\n",
    "3. **Quantization-Based Fine-Tuning (QLoRA)**:\n",
    "\n",
    "- Involves reducing the precision of model parameters (e.g., converting 32-bit floating-point values to 8-bit or 4-bit integers). This reduces the amount of CPU and GPU memory required by either 4x if using 8-bit integers, or 8x if using 4-bit integers.\n",
    "- Typically, since we're changing the weights to 8 or 4 bit integers, we will lose some precision/performance.\n",
    "- This can lead to reduced memory usage and faster inference on hardware with reduced precision support.\n",
    "- Particularly useful when deploying models on resource-constrained devices, such as mobile phones or edge devices.\n",
    "\n",
    "\n",
    "**Today, we're going to fine-tune using method 3 since we only have access to a single T4 GPU with 16GiB of GPU VRAM on Colab.** If you have more compute available, give LoRA based fine-tuning or full fine-tuning a try! Typically this requires 4 GPUs with 24GiB of GPU VRAM on a single node multi-GPU cluster and fine-tuning Deepspeeed.\n",
    "\n",
    "\n",
    "To do this, the new parameters we're introducing are:\n",
    "\n",
    "- `adapter`: The PEFT method we want to use\n",
    "- `quantization`: Load the weights in int4 or int8 to reduce memory overhead.\n",
    "- `trainer`: We enable the `finetune` trainer and can configure a variety of training parameters such as epochs and learning rate.\n",
    "\n",
    "Note, there are a few additional preprocessing parameters we should set to ensure that training runs smoothly:\n",
    "\n",
    "```yaml\n",
    "preprocessing:\n",
    "  global_max_sequence_length: 512\n",
    "  split:\n",
    "    type: random\n",
    "    probabilities:\n",
    "    - 1\n",
    "    - 0\n",
    "    - 0\n",
    "```\n",
    "\n",
    "- Some of the examples in the dataset have long sequences, so we set a `global_max_sequence_length` of 512 to ensure that we do not OOM.\n",
    "- Splits are set up such that we use all of the data for training as evaluation phases are synchronous and will take additional time. In a full training run, we recommend using setting aside some data for the test split for evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "cqbVueSLM5jK",
    "outputId": "d5f8c223-e416-4aa2-adb8-500e93138a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f7f836a3010> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f7d51644820, raw_cell=\"import torch\" store_history=True silent=False shell_futures=True cell_id=3f1fc8ef-576a-4be5-86cf-3dd1eeb45041>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b17a884f376a4494ae6a83afd18d34fd",
      "6b09d6bbb98e4a858ad319d3926c8744",
      "30ebd351ba2a45e8ad21adf6382a306d",
      "c121b8cb8f794fd0890275e8b9016743",
      "2390139f26ca4e3796de697185e2e9f1",
      "341d2775afc44554a20a7f21c96df269",
      "edab26fd431b4997b7a1bf7d85716387",
      "3ae8cbd461264f439ddd37c84e3df85f",
      "4303977564d3443a834651fa6cb92371",
      "b0453d5a80e54e3e8e207c4f42bc7d08",
      "bc4d7e1e366543dbbc4d52bab43ab7f6"
     ]
    },
    "id": "k-dtCIj73498",
    "outputId": "0a5730c4-e4b9-4a61-e150-5dd20e05598b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f7f836a3010> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f7d51c7fbb0, raw_cell=\"model = None\n",
      "clear_cache()\n",
      "\n",
      "qlora_fine_tuning_conf..\" store_history=True silent=False shell_futures=True cell_id=99fa871d-f17b-4dd2-a7ee-8ca1840c4c1c>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "╒════════════════════════╕\n",
      "│ EXPERIMENT DESCRIPTION │\n",
      "╘════════════════════════╛\n",
      "\n",
      "╒══════════════════╤═════════════════════════════════════════════════════════════════════════════════════════╕\n",
      "│ Experiment name  │ api_experiment                                                                          │\n",
      "├──────────────────┼─────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Model name       │ run                                                                                     │\n",
      "├──────────────────┼─────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Output directory │ /workspace/results/api_experiment_run_2                                                 │\n",
      "├──────────────────┼─────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ ludwig_version   │ '0.10.1.dev'                                                                            │\n",
      "├──────────────────┼─────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ command          │ ('/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py -f '                    │\n",
      "│                  │  '/root/.local/share/jupyter/runtime/kernel-a6272607-20b3-4d2e-b78d-6281916aad65.json') │\n",
      "├──────────────────┼─────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ random_seed      │ 42                                                                                      │\n",
      "├──────────────────┼─────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ data_format      │ \"<class 'pandas.core.frame.DataFrame'>\"                                                 │\n",
      "├──────────────────┼─────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ torch_version    │ '2.1.0+cu118'                                                                           │\n",
      "├──────────────────┼─────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ compute          │ {   'arch_list': [   'sm_50',                                                           │\n",
      "│                  │                      'sm_60',                                                           │\n",
      "│                  │                      'sm_70',                                                           │\n",
      "│                  │                      'sm_75',                                                           │\n",
      "│                  │                      'sm_80',                                                           │\n",
      "│                  │                      'sm_86',                                                           │\n",
      "│                  │                      'sm_37',                                                           │\n",
      "│                  │                      'sm_90'],                                                          │\n",
      "│                  │     'devices': {   0: {   'device_capability': (8, 6),                                  │\n",
      "│                  │                           'device_properties': \"_CudaDeviceProperties(name='NVIDIA \"    │\n",
      "│                  │                                                \"RTX A6000', major=8, minor=6, \"         │\n",
      "│                  │                                                'total_memory=48676MB, '                 │\n",
      "│                  │                                                'multi_processor_count=84)',             │\n",
      "│                  │                           'gpu_type': 'NVIDIA RTX A6000'}},                             │\n",
      "│                  │     'gencode_flags': '-gencode compute=compute_50,code=sm_50 -gencode '                 │\n",
      "│                  │                      'compute=compute_60,code=sm_60 -gencode '                          │\n",
      "│                  │                      'compute=compute_70,code=sm_70 -gencode '                          │\n",
      "│                  │                      'compute=compute_75,code=sm_75 -gencode '                          │\n",
      "│                  │                      'compute=compute_80,code=sm_80 -gencode '                          │\n",
      "│                  │                      'compute=compute_86,code=sm_86 -gencode '                          │\n",
      "│                  │                      'compute=compute_37,code=sm_37 -gencode '                          │\n",
      "│                  │                      'compute=compute_90,code=sm_90',                                   │\n",
      "│                  │     'gpus_per_node': 1,                                                                 │\n",
      "│                  │     'num_nodes': 1}                                                                     │\n",
      "╘══════════════════╧═════════════════════════════════════════════════════════════════════════════════════════╛\n",
      "\n",
      "╒═══════════════╕\n",
      "│ LUDWIG CONFIG │\n",
      "╘═══════════════╛\n",
      "\n",
      "User-specified config (with upgrades):\n",
      "\n",
      "{   'adapter': {'type': 'lora'},\n",
      "    'base_model': 'berkeley-nest/Starling-LM-7B-alpha',\n",
      "    'generation': {'max_new_tokens': 5000, 'temperature': 0.1},\n",
      "    'input_features': [{'name': 'input', 'type': 'text'}],\n",
      "    'ludwig_version': '0.10.1.dev',\n",
      "    'model_type': 'llm',\n",
      "    'output_features': [{'name': 'output', 'type': 'text'}],\n",
      "    'preprocessing': {   'global_max_sequence_length': 5000,\n",
      "                         'split': {   'probabilities': [1, 0, 0],\n",
      "                                      'type': 'random'}},\n",
      "    'prompt': {   'template': 'Below is an instruction that describes a task, '\n",
      "                              'paired with an input that provides further '\n",
      "                              'context. Write a response that appropriately '\n",
      "                              'completes the request.\\n'\n",
      "                              '### Instruction: {instruction}\\n'\n",
      "                              '### Input: {input}\\n'\n",
      "                              '### Response:'},\n",
      "    'quantization': {'bits': 4},\n",
      "    'trainer': {   'batch_size': 1,\n",
      "                   'epochs': 1,\n",
      "                   'eval_batch_size': 2,\n",
      "                   'gradient_accumulation_steps': 16,\n",
      "                   'learning_rate': 0.0004,\n",
      "                   'learning_rate_scheduler': {'warmup_fraction': 0.03},\n",
      "                   'type': 'finetune'}}\n",
      "\n",
      "Full config saved to:\n",
      "/workspace/results/api_experiment_run_2/api_experiment/model/model_hyperparameters.json\n",
      "\n",
      "╒═══════════════╕\n",
      "│ PREPROCESSING │\n",
      "╘═══════════════╛\n",
      "\n",
      "No cached dataset found at /workspace/f3d4d56ed75811ee97680242c0a83002.training.hdf5. Preprocessing the dataset.\n",
      "Using full dataframe\n",
      "Building dataset (it may take a while)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of berkeley-nest/Starling-LM-7B-alpha tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of feature 'None': 1358 (without start and stop symbols)\n",
      "Max sequence length is 1358 for feature 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of berkeley-nest/Starling-LM-7B-alpha tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of feature 'output': 726 (without start and stop symbols)\n",
      "Max sequence length is 726 for feature 'output'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of berkeley-nest/Starling-LM-7B-alpha tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of berkeley-nest/Starling-LM-7B-alpha tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset: DONE\n",
      "Writing preprocessed training set cache to /workspace/f3d4d56ed75811ee97680242c0a83002.training.hdf5\n",
      "Writing preprocessed validation set cache to /workspace/f3d4d56ed75811ee97680242c0a83002.validation.hdf5\n",
      "Writing preprocessed test set cache to /workspace/f3d4d56ed75811ee97680242c0a83002.test.hdf5\n",
      "Writing train set metadata to /workspace/f3d4d56ed75811ee97680242c0a83002.meta.json\n",
      "Validation set empty. If this is unintentional, please check the preprocessing configuration.\n",
      "Test set empty. If this is unintentional, please check the preprocessing configuration.\n",
      "\n",
      "Dataset Statistics\n",
      "╒═══════════╤═══════════════╤════════════════════╕\n",
      "│ Dataset   │   Size (Rows) │ Size (In Memory)   │\n",
      "╞═══════════╪═══════════════╪════════════════════╡\n",
      "│ Training  │          1879 │ 440.52 Kb          │\n",
      "╘═══════════╧═══════════════╧════════════════════╛\n",
      "\n",
      "╒═══════╕\n",
      "│ MODEL │\n",
      "╘═══════╛\n",
      "\n",
      "Warnings and other logs:\n",
      "Loading large language model...\n",
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efbc3282677e4b2492d160140d2e56fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of berkeley-nest/Starling-LM-7B-alpha tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Trainable Parameter Summary For Fine-Tuning\n",
      "Fine-tuning with adapter: lora\n",
      "trainable params: 3,407,872 || all params: 7,245,156,352 || trainable%: 0.047036555657757044\n",
      "==================================================\n",
      "\n",
      "╒══════════╕\n",
      "│ TRAINING │\n",
      "╘══════════╛\n",
      "\n",
      "Creating fresh model training run.\n",
      "Training for 1879 step(s), approximately 1 epoch(s).\n",
      "Early stopping policy: 5 round(s) of evaluation, or 9395 step(s), approximately 5 epoch(s).\n",
      "\n",
      "Starting with step 0, epoch: 0\n",
      "Training: 100%|██████████| 1879/1879 [29:43<00:00,  1.20s/it, loss=0.000617]\n",
      "Running evaluation for step: 1879, epoch: 1\n",
      "Evaluation took 0.2060s\n",
      "\n",
      "╒═══════════════════════╤════════════╤══════════════╤════════╕\n",
      "│                       │      train │ validation   │ test   │\n",
      "╞═══════════════════════╪════════════╪══════════════╪════════╡\n",
      "│ bleu                  │     0.0533 │              │        │\n",
      "├───────────────────────┼────────────┼──────────────┼────────┤\n",
      "│ char_error_rate       │     6.8877 │              │        │\n",
      "├───────────────────────┼────────────┼──────────────┼────────┤\n",
      "│ loss                  │     0.0760 │              │        │\n",
      "├───────────────────────┼────────────┼──────────────┼────────┤\n",
      "│ next_token_perplexity │ 12168.1895 │              │        │\n",
      "├───────────────────────┼────────────┼──────────────┼────────┤\n",
      "│ perplexity            │ 31436.2109 │              │        │\n",
      "├───────────────────────┼────────────┼──────────────┼────────┤\n",
      "│ rouge1_fmeasure       │     0.2482 │              │        │\n",
      "├───────────────────────┼────────────┼──────────────┼────────┤\n",
      "│ rouge1_precision      │     0.1465 │              │        │\n",
      "├───────────────────────┼────────────┼──────────────┼────────┤\n",
      "│ rouge1_recall         │     0.9829 │              │        │\n",
      "├───────────────────────┼────────────┼──────────────┼────────┤\n",
      "│ rouge2_fmeasure       │     0.2371 │              │        │\n",
      "├───────────────────────┼────────────┼──────────────┼────────┤\n",
      "│ rouge2_precision      │     0.1398 │              │        │\n",
      "├───────────────────────┼────────────┼──────────────┼────────┤\n",
      "│ rouge2_recall         │     0.9448 │              │        │\n",
      "├───────────────────────┼────────────┼──────────────┼────────┤\n",
      "│ rougeL_fmeasure       │     0.2430 │              │        │\n",
      "├───────────────────────┼────────────┼──────────────┼────────┤\n",
      "│ rougeL_precision      │     0.1434 │              │        │\n",
      "├───────────────────────┼────────────┼──────────────┼────────┤\n",
      "│ rougeL_recall         │     0.9609 │              │        │\n",
      "├───────────────────────┼────────────┼──────────────┼────────┤\n",
      "│ rougeLsum_fmeasure    │     0.2455 │              │        │\n",
      "├───────────────────────┼────────────┼──────────────┼────────┤\n",
      "│ rougeLsum_precision   │     0.1448 │              │        │\n",
      "├───────────────────────┼────────────┼──────────────┼────────┤\n",
      "│ rougeLsum_recall      │     0.9736 │              │        │\n",
      "├───────────────────────┼────────────┼──────────────┼────────┤\n",
      "│ sequence_accuracy     │     0.0000 │              │        │\n",
      "├───────────────────────┼────────────┼──────────────┼────────┤\n",
      "│ token_accuracy        │     0.0036 │              │        │\n",
      "├───────────────────────┼────────────┼──────────────┼────────┤\n",
      "│ word_error_rate       │    13.6918 │              │        │\n",
      "├───────────────────────┼────────────┼──────────────┼────────┤\n",
      "│ combined_loss         │     0.0760 │              │        │\n",
      "╘═══════════════════════╧════════════╧══════════════╧════════╛\n",
      "Saving model.\n",
      "\n",
      "Training: 100%|██████████| 1879/1879 [29:44<00:00,  1.05it/s, loss=0.000617]\n",
      "\n",
      "╒══════════╕\n",
      "│ FINISHED │\n",
      "╘══════════╛\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "clear_cache()\n",
    "\n",
    "qlora_fine_tuning_config = yaml.safe_load(\n",
    "\"\"\"\n",
    "model_type: llm\n",
    "base_model: berkeley-nest/Starling-LM-7B-alpha\n",
    "\n",
    "input_features:\n",
    "  - name: input\n",
    "    type: text\n",
    "\n",
    "output_features:\n",
    "  - name: output\n",
    "    type: text\n",
    "\n",
    "prompt:\n",
    "  template: >-\n",
    "    Below is an instruction that describes a task, paired with an input\n",
    "    that provides further context. Write a response that appropriately\n",
    "    completes the request.\n",
    "\n",
    "    ### Instruction: {instruction}\n",
    "\n",
    "    ### Input: {input}\n",
    "\n",
    "    ### Response:\n",
    "\n",
    "generation:\n",
    "  temperature: 0.1\n",
    "  max_new_tokens: 5000\n",
    "\n",
    "adapter:\n",
    "  type: lora\n",
    "\n",
    "quantization:\n",
    "  bits: 4\n",
    "\n",
    "preprocessing:\n",
    "  global_max_sequence_length: 5000\n",
    "  split:\n",
    "    type: random\n",
    "    probabilities:\n",
    "    - 1\n",
    "    - 0\n",
    "    - 0\n",
    "\n",
    "trainer:\n",
    "  type: finetune\n",
    "  epochs: 1\n",
    "  batch_size: 1\n",
    "  eval_batch_size: 2\n",
    "  gradient_accumulation_steps: 16\n",
    "  learning_rate: 0.0004\n",
    "  learning_rate_scheduler:\n",
    "    warmup_fraction: 0.03\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "model = LudwigModel(config=qlora_fine_tuning_config, logging_level=logging.INFO)\n",
    "results = model.train(dataset=main_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YkjcqMFMkwT"
   },
   "source": [
    "#### Perform Inference\n",
    "\n",
    "We can now use the model we fine-tuned above to make predictions on some test examples to see whether fine-tuning the large language model improve its ability to follow instructions/the tasks we're asking it to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f7f836a3010> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f7bafdd7b80, raw_cell=\"print(model)\" store_history=True silent=False shell_futures=True cell_id=abc8e85d-7cf5-46dc-9c83-e77ff0537903>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ludwig.api.LudwigModel object at 0x7f7f805b4b50>\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f7f836a3010> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f7bafd9fa90, raw_cell=\"test_examples = pd.DataFrame([\n",
      "    {\n",
      "        \"inst..\" store_history=True silent=False shell_futures=True cell_id=35fa2cd6-afd9-44e1-8dff-4ad8b74e4190>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of berkeley-nest/Starling-LM-7B-alpha tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 100%|██████████| 1/1 [00:07<00:00,  7.08s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of berkeley-nest/Starling-LM-7B-alpha tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting in: 8.35s.\n",
      "Instruction: \n",
      "          The task is to extract chemical-related triples from scientific research papers.\n",
      "          The rules are:\n",
      "          1. Only use the following predicates in the triple: “causes”, ”biolocation is”, “exposed through”, “sourced through”, “has role of”, “involved in”.\n",
      "          2. If there is more than one noun in the object, separate it into multiple triples.\n",
      "          3. If you don't find relevant chemical-related triples in the paper or you are not sure, return: null.\n",
      "          4. The response is an array of the relevant triples in the form: [subject, predicate, object].\n",
      "          Q: Interaction of TMPC with ANXA2 mediated attachment and colonization of S. anginosus and induced mitogen-activated protein kinase (MAPK) activation.\n",
      "          A: [\"TMPC\", \"involved in\", \"MAPK activation\"],\n",
      "          [\"ANXA2\", \"involved in\", \"MAPK activation\"]\n",
      "          Q: α-Lipoic acid plays an essential role in mitochondrial dehydrogenase reactions.\n",
      "          A: [\"alpha-Lipoic acid\", \"involved in\", \"mitochondrial dehydrogenase reactions\"]\n",
      "          Q: Ferroptosis, a form of regulated cell death that is driven by iron-dependent phospholipid peroxidation, has been implicated in multiple diseases, including cancer\n",
      "          A: [\"Ferroptosis\", \"causes\", \"cancer\"]\n",
      "          Q: These transporters and other biotin-binding proteins partition biotin to the cytoplasm and mitochondria cell compartments.\n",
      "          A: [\"Biotin\", \"biolocation is\", \"cytoplasm\"],\n",
      "          [\"Biotin\", \"biolocation is\", \"mitochondria\"]\n",
      "\n",
      "\n",
      "\n",
      "Input: Bioconversion of <Chemical>Chitin</Chemical> into <Chemical>chitin oligosaccharides</Chemical> using a novel chitinase with high <Chemical>Chitin</Chemical>-binding capacity. <Chemical>Chitin</Chemical> is the second largest renewable biomass resource in nature, it can be enzymatically degraded into high-value <Chemical>chitin oligosaccharides</Chemical> (<Chemical>CHOSs</Chemical>) by chitinases. In this study, a chitinase (ChiC8-1) was purified and biochemically characterized, its structure was analyzed by molecular modeling. ChiC8-1 had a molecular mass of approximately 96 kDa, exhibited its optimal activity at pH 6.0 and 50  C. The Km and Vmax values of ChiC8-1 towards colloidal <Chemical>Chitin</Chemical> were 10.17 mgmL-1 and 13.32 U/mg, respectively. Notably, ChiC8-1 showed high <Chemical>Chitin</Chemical>-binding capacity, which may be related to the two <Chemical>Chitin</Chemical> binding domains in the N-terminal. Based on the unique properties of ChiC8-1, a modified affinity chromatography method, which combines protein purification with <Chemical>Chitin</Chemical> hydrolysis process, was developed to purify ChiC8-1 while hydrolyzing <Chemical>Chitin</Chemical>. In this way, 9.36 +- 0.18 g <Chemical>CHOSs</Chemical> powder was directly obtained by hydrolyzing 10 g colloidal <Chemical>Chitin</Chemical> with crude enzyme solution. The <Chemical>CHOSs</Chemical> were composed of 14.77-2.83 % <Chemical>Acetylglucosamine</Chemical> and 85.23-97.17 % <Chemical>N N-diacetylchitobiose</Chemical> at different enzyme-substrate ratio. This process simplifies the tedious purification and separation steps, and may enable its potential application in the field of green production of <Chemical>chitin oligosaccharides</Chemical>.\n",
      "\n",
      "\n",
      "\n",
      "Generated Output: [['Chitin', 'involved in', 'Chitinase Binding'], ['ChiC8-1', 'involved in', 'Chitinase Binding'], ['Acetylglucosamine', 'involved in', 'Chitinase Binding'], ['N N-diacetylchitobiose', 'involved in', 'Chitinase Binding']]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ludwig/features/feature_utils.py:102: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(np.log(sequence_probabilities))\n"
     ]
    }
   ],
   "source": [
    "test_examples = pd.DataFrame([\n",
    "    {\n",
    "        \"instruction\" : \"\"\"\n",
    "          The task is to extract chemical-related triples from scientific research papers.\n",
    "          The rules are:\n",
    "          1. Only use the following predicates in the triple: “causes”, ”biolocation is”, “exposed through”, “sourced through”, “has role of”, “involved in”.\n",
    "          2. If there is more than one noun in the object, separate it into multiple triples.\n",
    "          3. If you don't find relevant chemical-related triples in the paper or you are not sure, return: null.\n",
    "          4. The response is an array of the relevant triples in the form: [subject, predicate, object].\n",
    "          Q: Interaction of TMPC with ANXA2 mediated attachment and colonization of S. anginosus and induced mitogen-activated protein kinase (MAPK) activation.\n",
    "          A: [\"TMPC\", \"involved in\", \"MAPK activation\"],\n",
    "          [\"ANXA2\", \"involved in\", \"MAPK activation\"]\n",
    "          Q: α-Lipoic acid plays an essential role in mitochondrial dehydrogenase reactions.\n",
    "          A: [\"alpha-Lipoic acid\", \"involved in\", \"mitochondrial dehydrogenase reactions\"]\n",
    "          Q: Ferroptosis, a form of regulated cell death that is driven by iron-dependent phospholipid peroxidation, has been implicated in multiple diseases, including cancer\n",
    "          A: [\"Ferroptosis\", \"causes\", \"cancer\"]\n",
    "          Q: These transporters and other biotin-binding proteins partition biotin to the cytoplasm and mitochondria cell compartments.\n",
    "          A: [\"Biotin\", \"biolocation is\", \"cytoplasm\"],\n",
    "          [\"Biotin\", \"biolocation is\", \"mitochondria\"]\"\"\",\n",
    "        \"input\": \"\"\"Bioconversion of <Chemical>Chitin</Chemical> into <Chemical>chitin oligosaccharides</Chemical> using a novel chitinase with high <Chemical>Chitin</Chemical>-binding capacity. <Chemical>Chitin</Chemical> is the second largest renewable biomass resource in nature, it can be enzymatically degraded into high-value <Chemical>chitin oligosaccharides</Chemical> (<Chemical>CHOSs</Chemical>) by chitinases. In this study, a chitinase (ChiC8-1) was purified and biochemically characterized, its structure was analyzed by molecular modeling. ChiC8-1 had a molecular mass of approximately 96 kDa, exhibited its optimal activity at pH 6.0 and 50  C. The Km and Vmax values of ChiC8-1 towards colloidal <Chemical>Chitin</Chemical> were 10.17 mgmL-1 and 13.32 U/mg, respectively. Notably, ChiC8-1 showed high <Chemical>Chitin</Chemical>-binding capacity, which may be related to the two <Chemical>Chitin</Chemical> binding domains in the N-terminal. Based on the unique properties of ChiC8-1, a modified affinity chromatography method, which combines protein purification with <Chemical>Chitin</Chemical> hydrolysis process, was developed to purify ChiC8-1 while hydrolyzing <Chemical>Chitin</Chemical>. In this way, 9.36 +- 0.18 g <Chemical>CHOSs</Chemical> powder was directly obtained by hydrolyzing 10 g colloidal <Chemical>Chitin</Chemical> with crude enzyme solution. The <Chemical>CHOSs</Chemical> were composed of 14.77-2.83 % <Chemical>Acetylglucosamine</Chemical> and 85.23-97.17 % <Chemical>N N-diacetylchitobiose</Chemical> at different enzyme-substrate ratio. This process simplifies the tedious purification and separation steps, and may enable its potential application in the field of green production of <Chemical>chitin oligosaccharides</Chemical>.\"\"\",\n",
    "    }])\n",
    "# test_examples = pd.DataFrame([\n",
    "#     {\n",
    "#         \"input\": \"Hi there.\",\n",
    "#     }])\n",
    "predictions = model.predict(test_examples)[0]\n",
    "for input_with_prediction in zip(test_examples['instruction'], test_examples['input'], predictions['output_response']):\n",
    "  print(f\"Instruction: {input_with_prediction[0]}\")\n",
    "  print(\"\\n\\n\")\n",
    "  print(f\"Input: {input_with_prediction[1]}\")\n",
    "  print(\"\\n\\n\")\n",
    "  print(f\"Generated Output: {input_with_prediction[2][0]}\")\n",
    "  print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alN-aOHQBLhH"
   },
   "source": [
    "#### **Observations From QLoRA Fine-Tuning** 🔎\n",
    "- Even when we just fine-tune the model on 100 examples from our dataset (which only takes about 4 minutes), it significantly improves the model on our task 🔥\n",
    "- The answers are not perfect when we just use 100 examples, but if we inspect the *logic* in the response, we can see that it is 95% of the way there. This is SIGNIFICANTLY better than before - there is no repetition and the actual code aspects of the answers are all correct.\n",
    "- The partial errors such as `sierp` instead of `arrray` etc indicate that we need to train on a larger amount of data for the model to better learn how to follow instructions and not make these kinds of mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVOUrJTBD2Io"
   },
   "source": [
    "If you're looking for a managed solution to handle all of the hassle of figuring out the right compute for your fine-tuning task, ensuring that they always succeed without CPU or GPU out-of-memory errors, and be able to rapidly deploy them for fast real-time inference, check out [Predibase](https://www.predibase.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f865bb5f6d0> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f83b874ba00, raw_cell=\"!export HUGGING_FACE_HUB_TOKEN=\"hf_ZLUKXLBDvYQnnHw..\" store_history=True silent=False shell_futures=True cell_id=c21d64a1-2131-4e0a-884e-776fec7ef654>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!export HUGGING_FACE_HUB_TOKEN=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "J5YTm1UgMp27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f7f836a3010> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f7bafdd6830, raw_cell=\"!ludwig upload hf_hub --repo_id mrmagic251/ChemSta..\" store_history=True silent=False shell_futures=True cell_id=9c6f15b0-ed89-40e2-bd9e-faa15700ac22>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/ludwig\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ludwig/cli.py\", line 197, in main\n",
      "    CLI()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ludwig/cli.py\", line 72, in __init__\n",
      "    getattr(self, args.command)()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ludwig/cli.py\", line 192, in upload\n",
      "    upload.cli(sys.argv[2:])\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ludwig/upload.py\", line 140, in cli\n",
      "    upload_cli(**vars(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ludwig/upload.py\", line 63, in upload_cli\n",
      "    hub = model_service()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ludwig/utils/upload_utils.py\", line 128, in __init__\n",
      "    self.login()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ludwig/utils/upload_utils.py\", line 133, in login\n",
      "    self.api = hf_hub_login()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ludwig/utils/upload_utils.py\", line 114, in hf_hub_login\n",
      "    login(add_to_git_credential=True)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/_login.py\", line 113, in login\n",
      "    interpreter_login(new_session=new_session, write_permission=write_permission)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/_login.py\", line 189, in interpreter_login\n",
      "    token = getpass(\"Enter your token (input will not be visible): \")\n",
      "  File \"/usr/lib/python3.10/getpass.py\", line 77, in unix_getpass\n",
      "    passwd = _raw_input(prompt, stream, input=input)\n",
      "  File \"/usr/lib/python3.10/getpass.py\", line 146, in _raw_input\n",
      "    line = input.readline()\n",
      "  File \"/usr/lib/python3.10/codecs.py\", line 319, in decode\n",
      "    def decode(self, input, final=False):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!ludwig upload hf_hub --repo_id mrmagic251/ChemStarling --model_path workspace/results/api_experiment_run_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "lgGfBBBjPT_7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f865bb5f6d0> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f83b874b070, raw_cell=\"from peft import PeftModel, PeftConfig\n",
      "from transf..\" store_history=True silent=False shell_futures=True cell_id=0decf902-e5e4-4c5b-85e2-9978681d8360>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't find 'adapter_config.json' at 'arnavgrg/codealpaca_v3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py:286\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/arnavgrg/codealpaca_v3/resolve/main/adapter_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/config.py:144\u001b[0m, in \u001b[0;36mPeftConfigMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     config_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhf_hub_download_kwargs\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1368\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1367\u001b[0m     \u001b[38;5;66;03m# Repo not found => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1368\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1370\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1238\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1238\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1631\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1631\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1639\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1640\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:385\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 385\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:409\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    408\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 409\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py:323\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    315\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    316\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 323\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-65d7f254-0708e1673db622765785690f;3a89be15-95d0-4a70-9eb8-39fc65012628)\n\nRepository Not Found for url: https://huggingface.co/arnavgrg/codealpaca_v3/resolve/main/adapter_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftModel, PeftConfig\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[0;32m----> 4\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mPeftConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marnavgrg/codealpaca_v3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-2-7b-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marnavgrg/codealpaca_v3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/config.py:148\u001b[0m, in \u001b[0;36mPeftConfigMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m         config_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    145\u001b[0m             pretrained_model_name_or_path, CONFIG_NAME, subfolder\u001b[38;5;241m=\u001b[39msubfolder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_hub_download_kwargs\n\u001b[1;32m    146\u001b[0m         )\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m--> 148\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    150\u001b[0m loaded_attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_json_file(config_file)\n\u001b[1;32m    151\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclass_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloaded_attributes}\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at 'arnavgrg/codealpaca_v3'"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "config = PeftConfig.from_pretrained(\"arnavgrg/codealpaca_v3\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "model = PeftModel.from_pretrained(model, \"arnavgrg/codealpaca_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "iG6x9y0QOjEh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f7f836a3010> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f7bafd9fca0, raw_cell=\"from ludwig.api import LudwigModel\n",
      "import pandas a..\" store_history=True silent=False shell_futures=True cell_id=02409a05-0dc6-4ae1-8f5e-d47112c7b92a>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "from ludwig.api import LudwigModel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f7f836a3010> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f7bafdd7d90, raw_cell=\"ludwig_model = LudwigModel.load(\"workspace/results..\" store_history=True silent=False shell_futures=True cell_id=226b2171-65aa-4d23-b726-2179209821b4>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/workspace/workspace/results/api_experiment_run_2/model/model_hyperparameters.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ludwig_model \u001b[38;5;241m=\u001b[39m \u001b[43mLudwigModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mworkspace/results/api_experiment_run_2/model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ludwig/api.py:1811\u001b[0m, in \u001b[0;36mLudwigModel.load\u001b[0;34m(model_dir, logging_level, backend, gpus, gpu_memory_limit, allow_parallel_threads, callbacks)\u001b[0m\n\u001b[1;32m   1806\u001b[0m backend \u001b[38;5;241m=\u001b[39m initialize_backend(backend)\n\u001b[1;32m   1807\u001b[0m backend\u001b[38;5;241m.\u001b[39minitialize_pytorch(\n\u001b[1;32m   1808\u001b[0m     gpus\u001b[38;5;241m=\u001b[39mgpus, gpu_memory_limit\u001b[38;5;241m=\u001b[39mgpu_memory_limit, allow_parallel_threads\u001b[38;5;241m=\u001b[39mallow_parallel_threads\n\u001b[1;32m   1809\u001b[0m )\n\u001b[0;32m-> 1811\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_return\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_HYPERPARAMETERS_FILE_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;66;03m# Upgrades deprecated fields and adds new required fields in case the config loaded from disk is old.\u001b[39;00m\n\u001b[1;32m   1814\u001b[0m config_obj \u001b[38;5;241m=\u001b[39m ModelConfig\u001b[38;5;241m.\u001b[39mfrom_dict(config)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ludwig/backend/base.py:222\u001b[0m, in \u001b[0;36mLocalTrainingMixin.broadcast_return\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbroadcast_return\u001b[39m(fn):\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ludwig/api.py:1811\u001b[0m, in \u001b[0;36mLudwigModel.load.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1806\u001b[0m backend \u001b[38;5;241m=\u001b[39m initialize_backend(backend)\n\u001b[1;32m   1807\u001b[0m backend\u001b[38;5;241m.\u001b[39minitialize_pytorch(\n\u001b[1;32m   1808\u001b[0m     gpus\u001b[38;5;241m=\u001b[39mgpus, gpu_memory_limit\u001b[38;5;241m=\u001b[39mgpu_memory_limit, allow_parallel_threads\u001b[38;5;241m=\u001b[39mallow_parallel_threads\n\u001b[1;32m   1809\u001b[0m )\n\u001b[0;32m-> 1811\u001b[0m config \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mbroadcast_return(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mload_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_HYPERPARAMETERS_FILE_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;66;03m# Upgrades deprecated fields and adds new required fields in case the config loaded from disk is old.\u001b[39;00m\n\u001b[1;32m   1814\u001b[0m config_obj \u001b[38;5;241m=\u001b[39m ModelConfig\u001b[38;5;241m.\u001b[39mfrom_dict(config)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ludwig/utils/data_utils.py:426\u001b[0m, in \u001b[0;36mload_json\u001b[0;34m(data_fp)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;129m@DeveloperAPI\u001b[39m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_json\u001b[39m(data_fp):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m open_file(data_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[1;32m    427\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(input_file)\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ludwig/utils/fs_utils.py:326\u001b[0m, in \u001b[0;36mopen_file\u001b[0;34m(url, *args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;129m@DeveloperAPI\u001b[39m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_file\u001b[39m(url, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    325\u001b[0m     fs, path \u001b[38;5;241m=\u001b[39m get_fs_and_path(url)\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fsspec/spec.py:1295\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1287\u001b[0m     mode \u001b[38;5;241m=\u001b[39m mode\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1289\u001b[0m     text_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1290\u001b[0m         k: kwargs\u001b[38;5;241m.\u001b[39mpop(k)\n\u001b[1;32m   1291\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnewline\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1292\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs\n\u001b[1;32m   1293\u001b[0m     }\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\n\u001b[0;32m-> 1295\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m            \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1303\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtext_kwargs,\n\u001b[1;32m   1304\u001b[0m     )\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1306\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fsspec/spec.py:1307\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1306\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[0;32m-> 1307\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fsspec/implementations/local.py:180\u001b[0m, in \u001b[0;36mLocalFileSystem._open\u001b[0;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_mkdir \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLocalFileOpener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fsspec/implementations/local.py:302\u001b[0m, in \u001b[0;36mLocalFileOpener.__init__\u001b[0;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;241m=\u001b[39m get_compression(path, compression)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[0;32m--> 302\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fsspec/implementations/local.py:307\u001b[0m, in \u001b[0;36mLocalFileOpener._open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mclosed:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m--> 307\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression:\n\u001b[1;32m    309\u001b[0m             compress \u001b[38;5;241m=\u001b[39m compr[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/workspace/workspace/results/api_experiment_run_2/model/model_hyperparameters.json'"
     ]
    }
   ],
   "source": [
    "ludwig_model = LudwigModel.load(\"workspace/results/api_experiment_run_2/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2390139f26ca4e3796de697185e2e9f1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30ebd351ba2a45e8ad21adf6382a306d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ae8cbd461264f439ddd37c84e3df85f",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4303977564d3443a834651fa6cb92371",
      "value": 3
     }
    },
    "341d2775afc44554a20a7f21c96df269": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ae8cbd461264f439ddd37c84e3df85f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4303977564d3443a834651fa6cb92371": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6b09d6bbb98e4a858ad319d3926c8744": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_341d2775afc44554a20a7f21c96df269",
      "placeholder": "​",
      "style": "IPY_MODEL_edab26fd431b4997b7a1bf7d85716387",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "b0453d5a80e54e3e8e207c4f42bc7d08": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b17a884f376a4494ae6a83afd18d34fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6b09d6bbb98e4a858ad319d3926c8744",
       "IPY_MODEL_30ebd351ba2a45e8ad21adf6382a306d",
       "IPY_MODEL_c121b8cb8f794fd0890275e8b9016743"
      ],
      "layout": "IPY_MODEL_2390139f26ca4e3796de697185e2e9f1"
     }
    },
    "bc4d7e1e366543dbbc4d52bab43ab7f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c121b8cb8f794fd0890275e8b9016743": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b0453d5a80e54e3e8e207c4f42bc7d08",
      "placeholder": "​",
      "style": "IPY_MODEL_bc4d7e1e366543dbbc4d52bab43ab7f6",
      "value": " 3/3 [01:09&lt;00:00, 22.62s/it]"
     }
    },
    "edab26fd431b4997b7a1bf7d85716387": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
