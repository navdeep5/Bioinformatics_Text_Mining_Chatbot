{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xb1aLHZRFrwA"
   },
   "source": [
    "# **Ludwig + DeepLearning.ai: Efficient Fine-Tuning for Llama2-7b on a Single GPU** ğŸ™Œ\n",
    "\n",
    "Let's explore how to fine-tune an LLM on a single commodity GPU with [Ludwig](https://ludwig.ai/latest/), an open-source package that empowers you to effortlessly build and train machine learning models like LLMs, neural networks and tree based models through declarative config files.\n",
    "\n",
    "In this notebook, we'll show an example of how to fine-tune Llama-2-7b to generate code using the CodeAlpaca dataset.\n",
    "\n",
    "By the end of this example, you will have gained a comprehensive understanding of the following key aspects:\n",
    "\n",
    "1. **Ludwig**: An intuitive toolkit that simplifies fine-tuning for open-source Language Model Models (LLMs).\n",
    "2. **Exploring the base model with prompts**: Dive into the intricacies of prompts and prompt templates, unlocking new dimensions in LLM interaction.\n",
    "3. **Fine-Tuning Large Language Models**: Navigate the world of model fine-tuning optimizations for getting the most out of a single memory-contrained GPU, including: LoRA and 4-bit quantization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXMVoEFkPXJJ"
   },
   "source": [
    "### **Install Ludwig and Ludwig's LLM related dependencies.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiZYaiRufHfh"
   },
   "source": [
    "Install Ludwig from the latest release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jvL1cL6wYtWz"
   },
   "outputs": [],
   "source": [
    "# !pip uninstall -y tensorflow --quiet\n",
    "# !pip install ludwig\n",
    "# !pip install ludwig[llm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7eAUs6efGH5"
   },
   "source": [
    "Install Ludwig from Ludwig master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yfYXyUUvM-R6",
    "outputId": "74e2cb69-f04d-485f-ef18-2db6c70619be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mDEPRECATION: git+https://github.com/ludwig-ai/ludwig.git@master#egg=ludwig[llm] contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y tensorflow --quiet\n",
    "!pip install git+https://github.com/ludwig-ai/ludwig.git@master --quiet\n",
    "!pip install \"git+https://github.com/ludwig-ai/ludwig.git@master#egg=ludwig[llm]\" --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2SRHcKAJYuu"
   },
   "source": [
    "Enable text wrapping so we don't have to scroll horizontally and create a function to flush CUDA cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ht4eVWxB13QL"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "\n",
    "get_ipython().events.register('pre_run_cell', set_css)\n",
    "\n",
    "def clear_cache():\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUkPQ9OLP0Po"
   },
   "source": [
    "### **Setup Your HuggingFace Token** ğŸ¤—\n",
    "\n",
    "We'll be exploring Llama-2 today, which a model released by Meta. However, the model is not openly-accessible and requires requesting for access (assigned to your HuggingFace token).\n",
    "\n",
    "Obtain a [HuggingFace API Token](https://huggingface.co/settings/tokens) and request access to [Llama2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf) before proceeding. You may need to signup on HuggingFace if you don't aleady have an account: https://huggingface.co/join\n",
    "\n",
    "Incase you haven't been given access to Llama-2-7b, that is alright. We can just use Llama-1 for the rest of this example: [huggyllama/llama-7b](https://huggingface.co/huggyllama/llama-7b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "P0AGbGklFKo3",
    "outputId": "a48dab25-d1f5-4587-fbfe-801e5d4b640a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f865bb5f6d0> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f865bbf26e0, raw_cell=\"# !pip uninstall torch -y\n",
      "# !pip install torch\n",
      "# i..\" store_history=True silent=False shell_futures=True cell_id=f8c8f927-991b-4fa4-8caa-4a995f113a26>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "# !pip uninstall torch -y\n",
    "# !pip install torch\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "GA89vvxMHzrF",
    "outputId": "b580a8ce-f3a8-4535-af94-ef044eb2e1a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f865bb5f6d0> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f865827c6d0, raw_cell=\"# # !pip uninstall torch torchtext -y\n",
      "# !pip insta..\" store_history=True silent=False shell_futures=True cell_id=39b95633-6624-47cc-99a1-9073be6f9209>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "# # !pip uninstall torch torchtext -y\n",
    "# !pip install torch torchtext\n",
    "# # !pip uninstall torchaudio -y\n",
    "# !pip install torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "nXOIxmfbQHSz",
    "outputId": "d2d43f0f-d32b-48c5-9404-a421af5bfac3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f865bb5f6d0> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f865827d1b0, raw_cell=\"import locale\n",
      "\n",
      "# Override getpreferredencoding to ..\" store_history=True silent=False shell_futures=True cell_id=69b4e9e7-40c0-49a7-956f-4b6096fa2a91>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.1.0+cu118 available.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Token: Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    }
   ],
   "source": [
    "import locale\n",
    "\n",
    "# Override getpreferredencoding to always return UTF-8\n",
    "locale.getpreferredencoding = lambda _=None: \"UTF-8\"\n",
    "\n",
    "import getpass\n",
    "# import locale; locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "import logging\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "from ludwig.api import LudwigModel\n",
    "\n",
    "\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = getpass.getpass(\"Token:\")\n",
    "assert os.environ[\"HUGGING_FACE_HUB_TOKEN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sV0D2rjRbk4z"
   },
   "source": [
    "### **Import The Code Generation Dataset** ğŸ“‹\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "cAsqyCDX-_NA",
    "outputId": "5abeeef7-11a4-4b47-e3e6-80c11c5f37a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f865bb5f6d0> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f841e054520, raw_cell=\"# from google.colab import drive\n",
      "# drive.mount('/c..\" store_history=True silent=False shell_futures=True cell_id=05e8c065-cf53-4fcb-96cf-3e6738504009>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "m8FXsCwYbkHh",
    "outputId": "dd4a0756-af11-4c3e-b1be-e95e775a927f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f865bb5f6d0> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f841e054400, raw_cell=\"# from google.colab import data_table; data_table...\" store_history=True silent=False shell_futures=True cell_id=e8e6dd4b-ab0e-487f-a5a6-8fd5cbe80ee9>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "# from google.colab import data_table; data_table.enable_dataframe_formatter()\n",
    "# import numpy as np; np.random.seed(123)\n",
    "# import pandas as pd\n",
    "\n",
    "# import json\n",
    "# with open('/content/drive/MyDrive/arxiv_physics_instruct_30k.jsonl') as f1:\n",
    "#     data1 = [json.loads(line) for line in f1]\n",
    "# with open('/content/drive/MyDrive/arxiv_math_instruct_50k.jsonl') as f2:\n",
    "#     data2 = [json.loads(line) for line in f2]\n",
    "\n",
    "# df1 = pd.DataFrame(data1)\n",
    "# df2 = pd.DataFrame(data2)\n",
    "# df = pd.concat([df1, df2])\n",
    "# main_df = df.sample(frac=1, random_state=42)\n",
    "# main_df.reset_index(drop=True, inplace=True)\n",
    "# # We're going to create a new column called `split` where:\n",
    "# # 90% will be assigned a value of 0 -> train set\n",
    "# # 5% will be assigned a value of 1 -> validation set\n",
    "# # 5% will be assigned a value of 2 -> test set\n",
    "# # Calculate the number of rows for each split value\n",
    "# total_rows = len(main_df)\n",
    "# split_0_count = int(total_rows * 0.9)\n",
    "# split_1_count = int(total_rows * 0.05)\n",
    "# split_2_count = total_rows - split_0_count - split_1_count\n",
    "\n",
    "# # Create an array with split values based on the counts\n",
    "# split_values = np.concatenate([\n",
    "#     np.zeros(split_0_count),\n",
    "#     np.ones(split_1_count),\n",
    "#     np.full(split_2_count, 2)\n",
    "# ])\n",
    "\n",
    "# # Shuffle the array to ensure randomness\n",
    "# np.random.shuffle(split_values)\n",
    "\n",
    "# # Add the 'split' column to the DataFrame\n",
    "# main_df['split'] = split_values\n",
    "# main_df['split'] = main_df['split'].astype(int)\n",
    "\n",
    "# # For this webinar, we will just 500 rows of this dataset.\n",
    "# main_df = main_df.head(n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2253
    },
    "id": "N-UW-zeKLsjZ",
    "outputId": "6760198b-04ad-4d1b-8631-d5f49293d584"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f865bb5f6d0> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f841e054940, raw_cell=\"# from google.colab import data_table; data_table...\" store_history=True silent=False shell_futures=True cell_id=d344b8b8-5fb9-4778-ac3e-0ee0506f2851>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "# from google.colab import data_table; data_table.enable_dataframe_formatter()\n",
    "# import numpy as np; np.random.seed(123)\n",
    "# import pandas as pd\n",
    "# import json\n",
    "\n",
    "# # Enable DataFrame formatter\n",
    "# data_table.enable_dataframe_formatter()\n",
    "\n",
    "# # Load data from the JSONL file\n",
    "# jsonl_file_path = 'train_data.jsonl'\n",
    "# with open(jsonl_file_path) as f:\n",
    "#     data = [json.loads(line) for line in f]\n",
    "\n",
    "# # Create DataFrame from the loaded data\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Shuffle the DataFrame\n",
    "# main_df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# # We're going to create a new column called `split` where:\n",
    "# # 90% will be assigned a value of 0 -> train set\n",
    "# # 5% will be assigned a value of 1 -> validation set\n",
    "# # 5% will be assigned a value of 2 -> test set\n",
    "\n",
    "# # Calculate the number of rows for each split value\n",
    "# total_rows = len(main_df)\n",
    "# split_0_count = int(total_rows * 0.9)\n",
    "# split_1_count = int(total_rows * 0.05)\n",
    "# split_2_count = total_rows - split_0_count - split_1_count\n",
    "\n",
    "# # Create an array with split values based on the counts\n",
    "# split_values = np.concatenate([\n",
    "#     np.zeros(split_0_count),\n",
    "#     np.ones(split_1_count),\n",
    "#     np.full(split_2_count, 2)\n",
    "# ])\n",
    "\n",
    "# # Shuffle the array to ensure randomness\n",
    "# np.random.shuffle(split_values)\n",
    "\n",
    "# # Add the 'split' column to the DataFrame\n",
    "# main_df['split'] = split_values.astype(int)\n",
    "\n",
    "# # Display a sample of the DataFrame\n",
    "# main_df.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f865bb5f6d0> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f841e054730, raw_cell=\"import numpy as np\n",
      "import pandas as pd\n",
      "import json..\" store_history=True silent=False shell_futures=True cell_id=c7c64c40-8b96-40d3-970b-3120e10b6c48>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>\\n\\nTitle: Metabolomic Analysis of Guinea Hen-...</td>\n",
       "      <td>[[LysoPE(18:2(9Z,12Z)/0:0), sourced through, G...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1362</th>\n",
       "      <td>\\n\\nTitle: Delphinidin 3-O-Glucosyl-5-O-Caffeo...</td>\n",
       "      <td>[[delphinidin 3-O-glucosyl-5-O-caffeoylglucosi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190</th>\n",
       "      <td>\\n\\nCytidine triphosphate (CTP) plays a crucia...</td>\n",
       "      <td>[[Cytidine triphosphate, involved in, Cardioli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290</th>\n",
       "      <td>\\n\\nTitle: Diverse Roles of PA(16:0/16:0) in L...</td>\n",
       "      <td>[[PA(16:0/16:0), involved in, Triacylglycerol ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>\\n\\nTitle: Exploring the Role of Lipid Transpo...</td>\n",
       "      <td>[[2,4,12-Octadecatrienoic acid isobutylamide, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  input  \\\n",
       "819   \\n\\nTitle: Metabolomic Analysis of Guinea Hen-...   \n",
       "1362  \\n\\nTitle: Delphinidin 3-O-Glucosyl-5-O-Caffeo...   \n",
       "1190  \\n\\nCytidine triphosphate (CTP) plays a crucia...   \n",
       "1290  \\n\\nTitle: Diverse Roles of PA(16:0/16:0) in L...   \n",
       "1155  \\n\\nTitle: Exploring the Role of Lipid Transpo...   \n",
       "\n",
       "                                                 output  split  \n",
       "819   [[LysoPE(18:2(9Z,12Z)/0:0), sourced through, G...      0  \n",
       "1362  [[delphinidin 3-O-glucosyl-5-O-caffeoylglucosi...      0  \n",
       "1190  [[Cytidine triphosphate, involved in, Cardioli...      0  \n",
       "1290  [[PA(16:0/16:0), involved in, Triacylglycerol ...      0  \n",
       "1155  [[2,4,12-Octadecatrienoic acid isobutylamide, ...      0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load data from the JSONL file\n",
    "jsonl_file_path = 'train_data.jsonl'\n",
    "with open(jsonl_file_path) as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# Create DataFrame from the loaded data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "main_df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# We're going to create a new column called `split` where:\n",
    "# 90% will be assigned a value of 0 -> train set\n",
    "# 5% will be assigned a value of 1 -> validation set\n",
    "# 5% will be assigned a value of 2 -> test set\n",
    "\n",
    "# Calculate the number of rows for each split value\n",
    "total_rows = len(main_df)\n",
    "split_0_count = int(total_rows * 0.9)\n",
    "split_1_count = int(total_rows * 0.05)\n",
    "split_2_count = total_rows - split_0_count - split_1_count\n",
    "\n",
    "# Create an array with split values based on the counts\n",
    "split_values = np.concatenate([\n",
    "    np.zeros(split_0_count),\n",
    "    np.ones(split_1_count),\n",
    "    np.full(split_2_count, 2)\n",
    "])\n",
    "\n",
    "# Shuffle the array to ensure randomness\n",
    "np.random.shuffle(split_values)\n",
    "\n",
    "# Add the 'split' column to the DataFrame\n",
    "main_df['split'] = split_values.astype(int)\n",
    "\n",
    "# Display a sample of the DataFrame\n",
    "main_df.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "bvfE3EZ677WT",
    "outputId": "6bf4e277-0961-4176-ae55-74a0f230c183"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f865bb5f6d0> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f841e0563b0, raw_cell=\"len(main_df)\" store_history=True silent=False shell_futures=True cell_id=dac8e7d7-f02a-4f09-bba3-18a2993dda78>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1879"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4211
    },
    "id": "Mx3JQMpmb1KQ",
    "outputId": "27472829-9f18-4007-9119-f8eaf79a6dc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f865bb5f6d0> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f841e057e50, raw_cell=\"main_df.head(n=10)\" store_history=True silent=False shell_futures=True cell_id=d94f07a8-f22e-48d9-8bba-33c807468c61>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nCytidine triphosphate (CTP) is a crucial m...</td>\n",
       "      <td>[[Cytidine triphosphate, involved in, Cardioli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nTitle: LysoPA(18:2(9Z,12Z)/0:0): A Key Pla...</td>\n",
       "      <td>[[LysoPA(18:2(9Z,12Z)/0:0), involved in, Cardi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nTitle: The Role of Neotussilagolactone, Et...</td>\n",
       "      <td>[[Neotussilagolactone, has role of, Surfactant...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nTitle: Sourcing L-Canaline and Related Com...</td>\n",
       "      <td>[[L-canaline, sourced through, Daikon radish],...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\nTitle: Phosphatidic Acid (PA) (16:0/18:1(1...</td>\n",
       "      <td>[[PA(16:0/18:1(11Z)), involved in, De Novo Tri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n\\nTitle: LysoPA(16:0/0:0) Involvement in Lip...</td>\n",
       "      <td>[[LysoPA(16:0/0:0), involved in, Cardiolipin B...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\n\\nTitle: Roles of Phosphate in De Novo Triac...</td>\n",
       "      <td>[[Phosphate, involved in, De Novo Triacylglyce...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n\\nTitle: Sources of Cis-Zeatin-9-N-Glucoside...</td>\n",
       "      <td>[[cis-Zeatin-9-N-glucoside, sourced through, I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\n\\nTitle: Palmityl-CoA: A Key Player in Lipid...</td>\n",
       "      <td>[[Palmityl-CoA, involved in, De Novo Triacylgl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\n\\nTitle: Exploring the Phytochemical Composi...</td>\n",
       "      <td>[[trans-p-Coumaric acid, sourced through, Bitt...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  \\n\\nCytidine triphosphate (CTP) is a crucial m...   \n",
       "1  \\n\\nTitle: LysoPA(18:2(9Z,12Z)/0:0): A Key Pla...   \n",
       "2  \\n\\nTitle: The Role of Neotussilagolactone, Et...   \n",
       "3  \\n\\nTitle: Sourcing L-Canaline and Related Com...   \n",
       "4  \\n\\nTitle: Phosphatidic Acid (PA) (16:0/18:1(1...   \n",
       "5  \\n\\nTitle: LysoPA(16:0/0:0) Involvement in Lip...   \n",
       "6  \\n\\nTitle: Roles of Phosphate in De Novo Triac...   \n",
       "7  \\n\\nTitle: Sources of Cis-Zeatin-9-N-Glucoside...   \n",
       "8  \\n\\nTitle: Palmityl-CoA: A Key Player in Lipid...   \n",
       "9  \\n\\nTitle: Exploring the Phytochemical Composi...   \n",
       "\n",
       "                                              output  split  \n",
       "0  [[Cytidine triphosphate, involved in, Cardioli...      1  \n",
       "1  [[LysoPA(18:2(9Z,12Z)/0:0), involved in, Cardi...      0  \n",
       "2  [[Neotussilagolactone, has role of, Surfactant...      0  \n",
       "3  [[L-canaline, sourced through, Daikon radish],...      0  \n",
       "4  [[PA(16:0/18:1(11Z)), involved in, De Novo Tri...      0  \n",
       "5  [[LysoPA(16:0/0:0), involved in, Cardiolipin B...      0  \n",
       "6  [[Phosphate, involved in, De Novo Triacylglyce...      0  \n",
       "7  [[cis-Zeatin-9-N-glucoside, sourced through, I...      0  \n",
       "8  [[Palmityl-CoA, involved in, De Novo Triacylgl...      1  \n",
       "9  [[trans-p-Coumaric acid, sourced through, Bitt...      0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eIyMPoSGFAN"
   },
   "source": [
    "As you can see below, the dataset is pretty balanced in terms of the number of examples of each type of instruction (also true for the full dataset with 20,000 rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "REqFr-ngx0et",
    "outputId": "7158cf56-3205-4fa1-a280-af6c96b6a0c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f865bb5f6d0> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f841e057400, raw_cell=\"# num_self_sufficient = (df['input'] == '').sum()\n",
      "..\" store_history=True silent=False shell_futures=True cell_id=622b6351-0366-4cf3-a90b-8924f1ef051d>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples in the dataset: 1879\n",
      "% of examples that are need additional context: 100.0\n"
     ]
    }
   ],
   "source": [
    "# num_self_sufficient = (df['input'] == '').sum()\n",
    "num_need_context = main_df.shape[0]\n",
    "# print(num_need_context)\n",
    "# We are only using 100 rows of this dataset for this webinar\n",
    "print(f\"Total number of examples in the dataset: {main_df.shape[0]}\")\n",
    "\n",
    "# print(f\"% of examples that are self-sufficient: {round(num_self_sufficient/main_df.shape[0] * 100, 2)}\")\n",
    "print(f\"% of examples that are need additional context: {round(num_need_context/main_df.shape[0] * 100, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcM8DVrIKXWj"
   },
   "source": [
    "The other aspect worth noting is the average number of characters in each of the three columns `instruction`, `input` and `output` in the dataset. Typically, every 3-4 characters maps to a *token* (the basic building blocks that language models use to understand and analyze text data), and large language models have a limit on the number of tokens they can take as input.\n",
    "\n",
    "The maximum context length for the base LLaMA-2 model is 4096 tokens. Ludwig automatically truncates texts that are too long for the model, but looking at these sequence lengths, we should be able to fine-tune on full length examples without needing any truncation.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "_3x3stdDKoCT",
    "outputId": "6da3dabc-1200-4851-f5c0-416b19b3e5f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f865bb5f6d0> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f841e0572e0, raw_cell=\"# # Calculating the length of each cell in each co..\" store_history=True silent=False shell_futures=True cell_id=ce2b9fd3-072d-456a-b50e-4413460d69eb>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "# # Calculating the length of each cell in each column\n",
    "# df['num_characters_question'] = df['question'].apply(lambda x: len(x))\n",
    "# # df['num_characters_input'] = df['input'].apply(lambda x: len(x))\n",
    "# df['num_characters_answer'] = df['answer'].apply(lambda x: len(x))\n",
    "\n",
    "# # Show Distribution\n",
    "# df.hist(column=['num_characters_question', 'num_characters_answer'])\n",
    "\n",
    "# # Calculating the average\n",
    "# average_chars_instruction = df['num_characters_question'].mean()\n",
    "# # average_chars_input = df['num_characters_input'].mean()\n",
    "# average_chars_output = df['num_characters_answer'].mean()\n",
    "\n",
    "# print(f'Average number of tokens in the instruction column: {(average_chars_instruction / 3):.0f}')\n",
    "# # print(f'Average number of tokens in the input column: {(average_chars_input / 3):.0f}')\n",
    "# print(f'Average number of tokens in the output column: {(average_chars_output / 3):.0f}', end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oU8iKtqwCc1b"
   },
   "source": [
    "There are three different fine-tuning approaches in Ludwig:\n",
    "\n",
    "1. **Full Fine-Tuning**:\n",
    "\n",
    "- Involves training the entire pre-trained model on new data from scratch.\n",
    "- All model layers and parameters are updated during fine-tuning.\n",
    "- Can lead to high accuracy but requires a significant amount of computational resources and time.\n",
    "- Runs the risk of catastrophic forgetting: occasionally, since we are updating all of the weights in the model, this process can lead to the algorithm inadvertently losing knowledge of its past tasks, i.e., the knowledge it gained during pretraining. The outcome may vary, with the algorithm experiencing heightened error margins in some cases, while in others, it might completely erase the memory of a specific task leading to terrible performance.\n",
    "- Best suited when the target task is significantly different from the original pre-training task.\n",
    "\n",
    "2. **Parameter Efficient Fine-Tuning (PEFT), e.g. LoRA**:\n",
    "\n",
    "- Focuses on updating only a subset of the model's parameters.\n",
    "- Often involves freezing certain layers or parts of the model to avoid catastrophic forgetting, or inserting additional layers that are trainable while keeping the original model's weights frozen.\n",
    "- Can result in faster fine-tuning with fewer computational resources, but might sacrifice some accuracy compared to full fine-tuning.\n",
    "- Includes methods like LoRA, AdaLoRA and Adaption Prompt (LLaMA Adapter)\n",
    "- Suitable when the new task shares similarities with the original pre-training task.\n",
    "\n",
    "3. **Quantization-Based Fine-Tuning (QLoRA)**:\n",
    "\n",
    "- Involves reducing the precision of model parameters (e.g., converting 32-bit floating-point values to 8-bit or 4-bit integers). This reduces the amount of CPU and GPU memory required by either 4x if using 8-bit integers, or 8x if using 4-bit integers.\n",
    "- Typically, since we're changing the weights to 8 or 4 bit integers, we will lose some precision/performance.\n",
    "- This can lead to reduced memory usage and faster inference on hardware with reduced precision support.\n",
    "- Particularly useful when deploying models on resource-constrained devices, such as mobile phones or edge devices.\n",
    "\n",
    "\n",
    "**Today, we're going to fine-tune using method 3 since we only have access to a single T4 GPU with 16GiB of GPU VRAM on Colab.** If you have more compute available, give LoRA based fine-tuning or full fine-tuning a try! Typically this requires 4 GPUs with 24GiB of GPU VRAM on a single node multi-GPU cluster and fine-tuning Deepspeeed.\n",
    "\n",
    "\n",
    "To do this, the new parameters we're introducing are:\n",
    "\n",
    "- `adapter`: The PEFT method we want to use\n",
    "- `quantization`: Load the weights in int4 or int8 to reduce memory overhead.\n",
    "- `trainer`: We enable the `finetune` trainer and can configure a variety of training parameters such as epochs and learning rate.\n",
    "\n",
    "Note, there are a few additional preprocessing parameters we should set to ensure that training runs smoothly:\n",
    "\n",
    "```yaml\n",
    "preprocessing:\n",
    "  global_max_sequence_length: 512\n",
    "  split:\n",
    "    type: random\n",
    "    probabilities:\n",
    "    - 1\n",
    "    - 0\n",
    "    - 0\n",
    "```\n",
    "\n",
    "- Some of the examples in the dataset have long sequences, so we set a `global_max_sequence_length` of 512 to ensure that we do not OOM.\n",
    "- Splits are set up such that we use all of the data for training as evaluation phases are synchronous and will take additional time. In a full training run, we recommend using setting aside some data for the test split for evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "cqbVueSLM5jK",
    "outputId": "d5f8c223-e416-4aa2-adb8-500e93138a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f865bb5f6d0> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f841e054730, raw_cell=\"import torch\" store_history=True silent=False shell_futures=True cell_id=a8452833-0bef-4bbd-9d4d-e36be87ac2d3>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b17a884f376a4494ae6a83afd18d34fd",
      "6b09d6bbb98e4a858ad319d3926c8744",
      "30ebd351ba2a45e8ad21adf6382a306d",
      "c121b8cb8f794fd0890275e8b9016743",
      "2390139f26ca4e3796de697185e2e9f1",
      "341d2775afc44554a20a7f21c96df269",
      "edab26fd431b4997b7a1bf7d85716387",
      "3ae8cbd461264f439ddd37c84e3df85f",
      "4303977564d3443a834651fa6cb92371",
      "b0453d5a80e54e3e8e207c4f42bc7d08",
      "bc4d7e1e366543dbbc4d52bab43ab7f6"
     ]
    },
    "id": "k-dtCIj73498",
    "outputId": "0a5730c4-e4b9-4a61-e150-5dd20e05598b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f865bb5f6d0> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f841e0562c0, raw_cell=\"model = None\n",
      "clear_cache()\n",
      "\n",
      "qlora_fine_tuning_conf..\" store_history=True silent=False shell_futures=True cell_id=41a0c938-0bd1-40fc-82bb-37519ac72b23>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
      "â”‚ EXPERIMENT DESCRIPTION â”‚\n",
      "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n",
      "\n",
      "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
      "â”‚ Experiment name  â”‚ api_experiment                                                                          â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ Model name       â”‚ run                                                                                     â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ Output directory â”‚ /workspace/results/api_experiment_run_1                                                 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ ludwig_version   â”‚ '0.10.dev'                                                                              â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ command          â”‚ ('/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py -f '                    â”‚\n",
      "â”‚                  â”‚  '/root/.local/share/jupyter/runtime/kernel-0bf6b53f-0a21-4249-b61f-ed1c08f2cfa0.json') â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ random_seed      â”‚ 42                                                                                      â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ data_format      â”‚ \"<class 'pandas.core.frame.DataFrame'>\"                                                 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ torch_version    â”‚ '2.1.0+cu118'                                                                           â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ compute          â”‚ {   'arch_list': [   'sm_50',                                                           â”‚\n",
      "â”‚                  â”‚                      'sm_60',                                                           â”‚\n",
      "â”‚                  â”‚                      'sm_70',                                                           â”‚\n",
      "â”‚                  â”‚                      'sm_75',                                                           â”‚\n",
      "â”‚                  â”‚                      'sm_80',                                                           â”‚\n",
      "â”‚                  â”‚                      'sm_86',                                                           â”‚\n",
      "â”‚                  â”‚                      'sm_37',                                                           â”‚\n",
      "â”‚                  â”‚                      'sm_90'],                                                          â”‚\n",
      "â”‚                  â”‚     'devices': {   0: {   'device_capability': (8, 6),                                  â”‚\n",
      "â”‚                  â”‚                           'device_properties': \"_CudaDeviceProperties(name='NVIDIA \"    â”‚\n",
      "â”‚                  â”‚                                                \"RTX A6000', major=8, minor=6, \"         â”‚\n",
      "â”‚                  â”‚                                                'total_memory=45625MB, '                 â”‚\n",
      "â”‚                  â”‚                                                'multi_processor_count=84)',             â”‚\n",
      "â”‚                  â”‚                           'gpu_type': 'NVIDIA RTX A6000'}},                             â”‚\n",
      "â”‚                  â”‚     'gencode_flags': '-gencode compute=compute_50,code=sm_50 -gencode '                 â”‚\n",
      "â”‚                  â”‚                      'compute=compute_60,code=sm_60 -gencode '                          â”‚\n",
      "â”‚                  â”‚                      'compute=compute_70,code=sm_70 -gencode '                          â”‚\n",
      "â”‚                  â”‚                      'compute=compute_75,code=sm_75 -gencode '                          â”‚\n",
      "â”‚                  â”‚                      'compute=compute_80,code=sm_80 -gencode '                          â”‚\n",
      "â”‚                  â”‚                      'compute=compute_86,code=sm_86 -gencode '                          â”‚\n",
      "â”‚                  â”‚                      'compute=compute_37,code=sm_37 -gencode '                          â”‚\n",
      "â”‚                  â”‚                      'compute=compute_90,code=sm_90',                                   â”‚\n",
      "â”‚                  â”‚     'gpus_per_node': 1,                                                                 â”‚\n",
      "â”‚                  â”‚     'num_nodes': 1}                                                                     â”‚\n",
      "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n",
      "\n",
      "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
      "â”‚ LUDWIG CONFIG â”‚\n",
      "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n",
      "\n",
      "User-specified config (with upgrades):\n",
      "\n",
      "{   'adapter': {'type': 'lora'},\n",
      "    'base_model': 'berkeley-nest/Starling-LM-7B-alpha',\n",
      "    'generation': {'max_new_tokens': 5000, 'temperature': 0.1},\n",
      "    'input_features': [{'name': 'input', 'type': 'text'}],\n",
      "    'ludwig_version': '0.10.dev',\n",
      "    'model_type': 'llm',\n",
      "    'output_features': [{'name': 'output', 'type': 'text'}],\n",
      "    'preprocessing': {   'global_max_sequence_length': 5000,\n",
      "                         'split': {   'probabilities': [1, 0, 0],\n",
      "                                      'type': 'random'}},\n",
      "    'prompt': {   'template': 'The task is to extract chemical-related triples '\n",
      "                              'from scientific research papers. The rules are: '\n",
      "                              '1. Only use the following predicates in the '\n",
      "                              'triple: â€œcausesâ€, â€biolocation isâ€, â€œexposed '\n",
      "                              'throughâ€, â€œsourced throughâ€, â€œhas role ofâ€, '\n",
      "                              'â€œinvolved inâ€. 2. If there is more than one '\n",
      "                              'noun in the object, separate it into multiple '\n",
      "                              \"triples. 3. If you don't find relevant \"\n",
      "                              'chemical-related triples in the paper or you '\n",
      "                              'are not sure, return: null. 4. The response is '\n",
      "                              'an array of the relevant triples in the form: '\n",
      "                              '[subject, predicate, object]. Q: Interaction of '\n",
      "                              'TMPC with ANXA2 mediated attachment and '\n",
      "                              'colonization of S. anginosus and induced '\n",
      "                              'mitogen-activated protein kinase (MAPK) '\n",
      "                              'activation. A: [\"TMPC\", \"involved in\", \"MAPK '\n",
      "                              'activation\"], [\"ANXA2\", \"involved in\", \"MAPK '\n",
      "                              'activation\"] Q: Î±-Lipoic acid plays an '\n",
      "                              'essential role in mitochondrial dehydrogenase '\n",
      "                              'reactions. A: [\"alpha-Lipoic acid\", \"involved '\n",
      "                              'in\", \"mitochondrial dehydrogenase reactions\"] '\n",
      "                              'Q: Ferroptosis, a form of regulated cell death '\n",
      "                              'that is driven by iron-dependent phospholipid '\n",
      "                              'peroxidation, has been implicated in multiple '\n",
      "                              'diseases, including cancer A: [\"Ferroptosis\", '\n",
      "                              '\"causes\", \"cancer\"] Q: These transporters and '\n",
      "                              'other biotin-binding proteins partition biotin '\n",
      "                              'to the cytoplasm and mitochondria cell '\n",
      "                              'compartments. A: [\"Biotin\", \"biolocation is\", '\n",
      "                              '\"cytoplasm\"], [\"Biotin\", \"biolocation is\", '\n",
      "                              '\"mitochondria\"] Q:\\n'\n",
      "                              '### question: {input}\\n'\n",
      "                              '\\n'\n",
      "                              '### answer:'},\n",
      "    'quantization': {'bits': 4},\n",
      "    'trainer': {   'batch_size': 1,\n",
      "                   'epochs': 3,\n",
      "                   'eval_batch_size': 2,\n",
      "                   'gradient_accumulation_steps': 16,\n",
      "                   'learning_rate': 0.0004,\n",
      "                   'learning_rate_scheduler': {'warmup_fraction': 0.03},\n",
      "                   'type': 'finetune'}}\n",
      "\n",
      "Full config saved to:\n",
      "/workspace/results/api_experiment_run_1/api_experiment/model/model_hyperparameters.json\n",
      "\n",
      "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
      "â”‚ PREPROCESSING â”‚\n",
      "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n",
      "\n",
      "No cached dataset found at /workspace/8a1d4d60d1e811ee8ce10242c0a8fd02.training.hdf5. Preprocessing the dataset.\n",
      "Using full dataframe\n",
      "Building dataset (it may take a while)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of berkeley-nest/Starling-LM-7B-alpha tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of feature 'None': 1175 (without start and stop symbols)\n",
      "Max sequence length is 1175 for feature 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of berkeley-nest/Starling-LM-7B-alpha tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of feature 'output': 516 (without start and stop symbols)\n",
      "Max sequence length is 516 for feature 'output'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of berkeley-nest/Starling-LM-7B-alpha tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of berkeley-nest/Starling-LM-7B-alpha tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset: DONE\n",
      "Writing preprocessed training set cache to /workspace/8a1d4d60d1e811ee8ce10242c0a8fd02.training.hdf5\n",
      "Writing preprocessed validation set cache to /workspace/8a1d4d60d1e811ee8ce10242c0a8fd02.validation.hdf5\n",
      "Writing preprocessed test set cache to /workspace/8a1d4d60d1e811ee8ce10242c0a8fd02.test.hdf5\n",
      "Writing train set metadata to /workspace/8a1d4d60d1e811ee8ce10242c0a8fd02.meta.json\n",
      "Validation set empty. If this is unintentional, please check the preprocessing configuration.\n",
      "Test set empty. If this is unintentional, please check the preprocessing configuration.\n",
      "\n",
      "Dataset Statistics\n",
      "â•’â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
      "â”‚ Dataset   â”‚   Size (Rows) â”‚ Size (In Memory)   â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ Training  â”‚           100 â”‚ 23.56 Kb           â”‚\n",
      "â•˜â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n",
      "\n",
      "â•’â•â•â•â•â•â•â•â••\n",
      "â”‚ MODEL â”‚\n",
      "â•˜â•â•â•â•â•â•â•â•›\n",
      "\n",
      "Warnings and other logs:\n",
      "Loading large language model...\n",
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccec01ee1f0f4c81876e6bb0ffa1c5a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of berkeley-nest/Starling-LM-7B-alpha tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Trainable Parameter Summary For Fine-Tuning\n",
      "Fine-tuning with adapter: lora\n",
      "trainable params: 3,407,872 || all params: 7,245,156,352 || trainable%: 0.047036555657757044\n",
      "==================================================\n",
      "\n",
      "â•’â•â•â•â•â•â•â•â•â•â•â••\n",
      "â”‚ TRAINING â”‚\n",
      "â•˜â•â•â•â•â•â•â•â•â•â•â•›\n",
      "\n",
      "Creating fresh model training run.\n",
      "Training for 300 step(s), approximately 3 epoch(s).\n",
      "Early stopping policy: 5 round(s) of evaluation, or 500 step(s), approximately 5 epoch(s).\n",
      "\n",
      "Starting with step 0, epoch: 0\n",
      "Training:  33%|â–ˆâ–ˆâ–ˆâ–      | 100/300 [01:45<03:55,  1.18s/it, loss=0.00469]\n",
      "Running evaluation for step: 100, epoch: 1\n",
      "Evaluation took 0.9091s\n",
      "\n",
      "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â••\n",
      "â”‚                       â”‚      train â”‚ validation   â”‚ test   â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ bleu                  â”‚     0.0426 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ char_error_rate       â”‚     7.4933 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ loss                  â”‚     0.1667 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ next_token_perplexity â”‚ 12502.3311 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ perplexity            â”‚ 31686.0312 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rouge1_fmeasure       â”‚     0.2229 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rouge1_precision      â”‚     0.1311 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rouge1_recall         â”‚     0.9793 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rouge2_fmeasure       â”‚     0.2097 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rouge2_precision      â”‚     0.1235 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rouge2_recall         â”‚     0.9179 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rougeL_fmeasure       â”‚     0.2166 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rougeL_precision      â”‚     0.1275 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rougeL_recall         â”‚     0.9455 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rougeLsum_fmeasure    â”‚     0.2204 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rougeLsum_precision   â”‚     0.1296 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rougeLsum_recall      â”‚     0.9701 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ sequence_accuracy     â”‚     0.0000 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ token_accuracy        â”‚     0.0020 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ word_error_rate       â”‚    14.0319 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ combined_loss         â”‚     0.1667 â”‚              â”‚        â”‚\n",
      "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•›\n",
      "Saving model.\n",
      "\n",
      "Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 200/300 [03:31<02:18,  1.38s/it, loss=0.001]   \n",
      "Running evaluation for step: 200, epoch: 2\n",
      "Evaluation took 0.6067s\n",
      "\n",
      "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â••\n",
      "â”‚                       â”‚      train â”‚ validation   â”‚ test   â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ bleu                  â”‚     0.0661 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ char_error_rate       â”‚     5.9878 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ loss                  â”‚     0.0302 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ next_token_perplexity â”‚ 11991.3877 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ perplexity            â”‚ 31568.4316 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rouge1_fmeasure       â”‚     0.2941 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rouge1_precision      â”‚     0.1754 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rouge1_recall         â”‚     0.9974 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rouge2_fmeasure       â”‚     0.2841 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rouge2_precision      â”‚     0.1693 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rouge2_recall         â”‚     0.9671 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rougeL_fmeasure       â”‚     0.2907 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rougeL_precision      â”‚     0.1734 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rougeL_recall         â”‚     0.9845 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rougeLsum_fmeasure    â”‚     0.2921 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rougeLsum_precision   â”‚     0.1742 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rougeLsum_recall      â”‚     0.9903 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ sequence_accuracy     â”‚     0.0000 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ token_accuracy        â”‚     0.0032 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ word_error_rate       â”‚    11.8112 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ combined_loss         â”‚     0.0302 â”‚              â”‚        â”‚\n",
      "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•›\n",
      "Saving model.\n",
      "\n",
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [05:18<00:00,  1.34s/it, loss=0.00311] \n",
      "Running evaluation for step: 300, epoch: 3\n",
      "Evaluation took 0.1063s\n",
      "\n",
      "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â••\n",
      "â”‚                       â”‚      train â”‚ validation   â”‚ test   â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ bleu                  â”‚     0.0520 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ char_error_rate       â”‚     7.7569 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ loss                  â”‚     0.0393 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ next_token_perplexity â”‚ 11966.2920 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ perplexity            â”‚ 31358.2715 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rouge1_fmeasure       â”‚     0.2177 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rouge1_precision      â”‚     0.1257 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rouge1_recall         â”‚     0.9824 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rouge2_fmeasure       â”‚     0.2113 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rouge2_precision      â”‚     0.1219 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rouge2_recall         â”‚     0.9568 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rougeL_fmeasure       â”‚     0.2177 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rougeL_precision      â”‚     0.1257 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rougeL_recall         â”‚     0.9824 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rougeLsum_fmeasure    â”‚     0.2177 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rougeLsum_precision   â”‚     0.1257 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ rougeLsum_recall      â”‚     0.9824 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ sequence_accuracy     â”‚     0.0000 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ token_accuracy        â”‚     0.0037 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ word_error_rate       â”‚    14.9563 â”‚              â”‚        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ combined_loss         â”‚     0.0393 â”‚              â”‚        â”‚\n",
      "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•›\n",
      "Saving model.\n",
      "\n",
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [05:20<00:00,  1.07s/it, loss=0.00311]\n",
      "\n",
      "â•’â•â•â•â•â•â•â•â•â•â•â••\n",
      "â”‚ FINISHED â”‚\n",
      "â•˜â•â•â•â•â•â•â•â•â•â•â•›\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "clear_cache()\n",
    "\n",
    "qlora_fine_tuning_config = yaml.safe_load(\n",
    "\"\"\"\n",
    "model_type: llm\n",
    "base_model: berkeley-nest/Starling-LM-7B-alpha\n",
    "\n",
    "input_features:\n",
    "  - name: input\n",
    "    type: text\n",
    "\n",
    "output_features:\n",
    "  - name: output\n",
    "    type: text\n",
    "\n",
    "prompt:\n",
    "  template: >-\n",
    "    The task is to extract chemical-related triples from scientific research papers.\n",
    "    The rules are:\n",
    "    1. Only use the following predicates in the triple: â€œcausesâ€, â€biolocation isâ€, â€œexposed throughâ€, â€œsourced throughâ€, â€œhas role ofâ€, â€œinvolved inâ€.\n",
    "    2. If there is more than one noun in the object, separate it into multiple triples.\n",
    "    3. If you don't find relevant chemical-related triples in the paper or you are not sure, return: null.\n",
    "    4. The response is an array of the relevant triples in the form: [subject, predicate, object].\n",
    "    Q: Interaction of TMPC with ANXA2 mediated attachment and colonization of S. anginosus and induced mitogen-activated protein kinase (MAPK) activation.\n",
    "    A: [\"TMPC\", \"involved in\", \"MAPK activation\"],\n",
    "    [\"ANXA2\", \"involved in\", \"MAPK activation\"]\n",
    "    Q: Î±-Lipoic acid plays an essential role in mitochondrial dehydrogenase reactions.\n",
    "    A: [\"alpha-Lipoic acid\", \"involved in\", \"mitochondrial dehydrogenase reactions\"]\n",
    "    Q: Ferroptosis, a form of regulated cell death that is driven by iron-dependent phospholipid peroxidation, has been implicated in multiple diseases, including cancer\n",
    "    A: [\"Ferroptosis\", \"causes\", \"cancer\"]\n",
    "    Q: These transporters and other biotin-binding proteins partition biotin to the cytoplasm and mitochondria cell compartments.\n",
    "    A: [\"Biotin\", \"biolocation is\", \"cytoplasm\"],\n",
    "    [\"Biotin\", \"biolocation is\", \"mitochondria\"]\n",
    "    Q:\n",
    "\n",
    "    ### question: {input}\n",
    "\n",
    "\n",
    "    ### answer:\n",
    "\n",
    "generation:\n",
    "  temperature: 0.1\n",
    "  max_new_tokens: 5000\n",
    "\n",
    "adapter:\n",
    "  type: lora\n",
    "\n",
    "quantization:\n",
    "  bits: 4\n",
    "\n",
    "preprocessing:\n",
    "  global_max_sequence_length: 5000\n",
    "  split:\n",
    "    type: random\n",
    "    probabilities:\n",
    "    - 1\n",
    "    - 0\n",
    "    - 0\n",
    "\n",
    "trainer:\n",
    "  type: finetune\n",
    "  epochs: 3\n",
    "  batch_size: 1\n",
    "  eval_batch_size: 2\n",
    "  gradient_accumulation_steps: 16\n",
    "  learning_rate: 0.0004\n",
    "  learning_rate_scheduler:\n",
    "    warmup_fraction: 0.03\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "model = LudwigModel(config=qlora_fine_tuning_config, logging_level=logging.INFO)\n",
    "results = model.train(dataset=main_df[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YkjcqMFMkwT"
   },
   "source": [
    "#### Perform Inference\n",
    "\n",
    "We can now use the model we fine-tuned above to make predictions on some test examples to see whether fine-tuning the large language model improve its ability to follow instructions/the tasks we're asking it to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f865bb5f6d0> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f83b874a410, raw_cell=\"print(model)\" store_history=True silent=False shell_futures=True cell_id=7301e820-7b8d-4ccd-9fcf-f96b601997ea>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ludwig.api.LudwigModel object at 0x7f865827c2e0>\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f865bb5f6d0> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f863130f550, raw_cell=\"test_examples = pd.DataFrame([\n",
      "    {\n",
      "        \"inpu..\" store_history=True silent=False shell_futures=True cell_id=919d6824-86d7-4088-ac6f-f2c9d3cedc23>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of berkeley-nest/Starling-LM-7B-alpha tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:11<00:00, 11.97s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of berkeley-nest/Starling-LM-7B-alpha tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting in: 13.65s.\n",
      "Instruction: Title: Diverse Sources of Polyunsaturated Fatty Acids and Related Metabolites in Vanilla and Fruits\n",
      "\n",
      "Background: Polyunsaturated fatty acids (PUFAs) and their related metabolites play essential roles in various biological processes. In this study, we investigated the sources of PUFAs and their related metabolites in Vanilla and fruits.\n",
      "\n",
      "Methods: We analyzed the composition of PUFAs and related metabolites in Vanilla and fruits using gas chromatography-mass spectrometry (GC-MS). The sources of PUFAs and related metabolites were identified by comparing their retention times and mass spectra with those of authentic standards.\n",
      "\n",
      "Results: We found that Vanilla contained significant amounts of PA(18:3(9Z,12Z,15Z)/20:1(11Z)) and homogentisate. In contrast, Lowbush blueberry was rich in PA(18:3(9Z,12Z,15Z)/20:1(11Z)) and 5-Hydroxyisourate. Pak choy was found to be a source of PA(18:3(9Z,12Z,15Z)/20:1(11Z)).\n",
      "\n",
      "Conclusion: Our findings highlight the diverse sources of PUFAs and related metabolites in Vanilla and fruits. These results contribute to our understanding of the potential health benefits of consuming these plants and their products. Further research is needed to elucidate the specific roles of these compounds in human health and their potential applications in functional foods and nutraceuticals.\n",
      "\n",
      "\n",
      "\n",
      "Generated Output: [['PA(18:3(9Z,12Z,15Z)/20:1(11Z))', 'sourced through', 'Vanilla'], ['homogentisate', 'sourced through', 'Vanilla'], ['PA(18:3(9Z,12Z,15Z)/20:1(11Z))', 'sourced through', 'Lowbush blueberry'], ['5-Hydroxyisourate', 'sourced through', 'Lowbush blueberry'], ['PA(18:3(9Z,12Z,15Z)/20:1(11Z))', 'sourced through', 'Pak choy']]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ludwig/features/feature_utils.py:102: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(np.log(sequence_probabilities))\n"
     ]
    }
   ],
   "source": [
    "test_examples = pd.DataFrame([\n",
    "    {\n",
    "        \"input\": \"\"\"Title: Diverse Sources of Polyunsaturated Fatty Acids and Related Metabolites in Vanilla and Fruits\\n\\nBackground: Polyunsaturated fatty acids (PUFAs) and their related metabolites play essential roles in various biological processes. In this study, we investigated the sources of PUFAs and their related metabolites in Vanilla and fruits.\\n\\nMethods: We analyzed the composition of PUFAs and related metabolites in Vanilla and fruits using gas chromatography-mass spectrometry (GC-MS). The sources of PUFAs and related metabolites were identified by comparing their retention times and mass spectra with those of authentic standards.\\n\\nResults: We found that Vanilla contained significant amounts of PA(18:3(9Z,12Z,15Z)/20:1(11Z)) and homogentisate. In contrast, Lowbush blueberry was rich in PA(18:3(9Z,12Z,15Z)/20:1(11Z)) and 5-Hydroxyisourate. Pak choy was found to be a source of PA(18:3(9Z,12Z,15Z)/20:1(11Z)).\\n\\nConclusion: Our findings highlight the diverse sources of PUFAs and related metabolites in Vanilla and fruits. These results contribute to our understanding of the potential health benefits of consuming these plants and their products. Further research is needed to elucidate the specific roles of these compounds in human health and their potential applications in functional foods and nutraceuticals.\"\"\",\n",
    "    }])\n",
    "# test_examples = pd.DataFrame([\n",
    "#     {\n",
    "#         \"input\": \"Hi there.\",\n",
    "#     }])\n",
    "predictions = model.predict(test_examples)[0]\n",
    "for input_with_prediction in zip(test_examples['input'], predictions['output_response']):\n",
    "  print(f\"Instruction: {input_with_prediction[0]}\")\n",
    "  print(\"\\n\\n\")\n",
    "  print(f\"Generated Output: {input_with_prediction[1][0]}\")\n",
    "  print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alN-aOHQBLhH"
   },
   "source": [
    "#### **Observations From QLoRA Fine-Tuning** ğŸ”\n",
    "- Even when we just fine-tune the model on 100 examples from our dataset (which only takes about 4 minutes), it significantly improves the model on our task ğŸ”¥\n",
    "- The answers are not perfect when we just use 100 examples, but if we inspect the *logic* in the response, we can see that it is 95% of the way there. This is SIGNIFICANTLY better than before - there is no repetition and the actual code aspects of the answers are all correct.\n",
    "- The partial errors such as `sierp` instead of `arrray` etc indicate that we need to train on a larger amount of data for the model to better learn how to follow instructions and not make these kinds of mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVOUrJTBD2Io"
   },
   "source": [
    "If you're looking for a managed solution to handle all of the hassle of figuring out the right compute for your fine-tuning task, ensuring that they always succeed without CPU or GPU out-of-memory errors, and be able to rapidly deploy them for fast real-time inference, check out [Predibase](https://www.predibase.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f865bb5f6d0> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f83b874ba00, raw_cell=\"!export HUGGING_FACE_HUB_TOKEN=\"hf_ZLUKXLBDvYQnnHw..\" store_history=True silent=False shell_futures=True cell_id=c21d64a1-2131-4e0a-884e-776fec7ef654>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!export HUGGING_FACE_HUB_TOKEN=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "J5YTm1UgMp27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f865bb5f6d0> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f83b874bc70, raw_cell=\"!ludwig upload hf_hub --repo_id mrmagic251/ChemSta..\" store_history=True silent=False shell_futures=True cell_id=5a771c66-a784-47f6-9cd9-ffbf3bf68f00>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Token: Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/ludwig\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ludwig/cli.py\", line 197, in main\n",
      "    CLI()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ludwig/cli.py\", line 72, in __init__\n",
      "    getattr(self, args.command)()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ludwig/cli.py\", line 192, in upload\n",
      "    upload.cli(sys.argv[2:])\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ludwig/upload.py\", line 140, in cli\n",
      "    upload_cli(**vars(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ludwig/upload.py\", line 63, in upload_cli\n",
      "    hub = model_service()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ludwig/utils/upload_utils.py\", line 128, in __init__\n",
      "    self.login()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ludwig/utils/upload_utils.py\", line 133, in login\n",
      "    self.api = hf_hub_login()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ludwig/utils/upload_utils.py\", line 114, in hf_hub_login\n",
      "    login(add_to_git_credential=True)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/_login.py\", line 113, in login\n",
      "    interpreter_login(new_session=new_session, write_permission=write_permission)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/_login.py\", line 189, in interpreter_login\n",
      "    token = getpass(\"Token: \")\n",
      "  File \"/usr/lib/python3.10/getpass.py\", line 77, in unix_getpass\n",
      "    passwd = _raw_input(prompt, stream, input=input)\n",
      "  File \"/usr/lib/python3.10/getpass.py\", line 146, in _raw_input\n",
      "    line = input.readline()\n",
      "  File \"/usr/lib/python3.10/codecs.py\", line 319, in decode\n",
      "    def decode(self, input, final=False):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!ludwig upload hf_hub --repo_id mrmagic251/ChemStarling --model_path /results/api_experiment_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "lgGfBBBjPT_7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function set_css at 0x7f865bb5f6d0> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f83b874b070, raw_cell=\"from peft import PeftModel, PeftConfig\n",
      "from transf..\" store_history=True silent=False shell_futures=True cell_id=0decf902-e5e4-4c5b-85e2-9978681d8360>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_css() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: set_css() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't find 'adapter_config.json' at 'arnavgrg/codealpaca_v3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py:286\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/arnavgrg/codealpaca_v3/resolve/main/adapter_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/config.py:144\u001b[0m, in \u001b[0;36mPeftConfigMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     config_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhf_hub_download_kwargs\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1368\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1367\u001b[0m     \u001b[38;5;66;03m# Repo not found => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1368\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1370\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1238\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1238\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1631\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1631\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1639\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1640\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:385\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 385\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:409\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    408\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 409\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py:323\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    315\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    316\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 323\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-65d7f254-0708e1673db622765785690f;3a89be15-95d0-4a70-9eb8-39fc65012628)\n\nRepository Not Found for url: https://huggingface.co/arnavgrg/codealpaca_v3/resolve/main/adapter_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftModel, PeftConfig\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[0;32m----> 4\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mPeftConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marnavgrg/codealpaca_v3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-2-7b-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marnavgrg/codealpaca_v3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/config.py:148\u001b[0m, in \u001b[0;36mPeftConfigMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m         config_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    145\u001b[0m             pretrained_model_name_or_path, CONFIG_NAME, subfolder\u001b[38;5;241m=\u001b[39msubfolder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_hub_download_kwargs\n\u001b[1;32m    146\u001b[0m         )\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m--> 148\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    150\u001b[0m loaded_attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_json_file(config_file)\n\u001b[1;32m    151\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclass_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloaded_attributes}\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at 'arnavgrg/codealpaca_v3'"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "config = PeftConfig.from_pretrained(\"arnavgrg/codealpaca_v3\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "model = PeftModel.from_pretrained(model, \"arnavgrg/codealpaca_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iG6x9y0QOjEh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2390139f26ca4e3796de697185e2e9f1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30ebd351ba2a45e8ad21adf6382a306d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ae8cbd461264f439ddd37c84e3df85f",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4303977564d3443a834651fa6cb92371",
      "value": 3
     }
    },
    "341d2775afc44554a20a7f21c96df269": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ae8cbd461264f439ddd37c84e3df85f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4303977564d3443a834651fa6cb92371": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6b09d6bbb98e4a858ad319d3926c8744": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_341d2775afc44554a20a7f21c96df269",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_edab26fd431b4997b7a1bf7d85716387",
      "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
     }
    },
    "b0453d5a80e54e3e8e207c4f42bc7d08": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b17a884f376a4494ae6a83afd18d34fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6b09d6bbb98e4a858ad319d3926c8744",
       "IPY_MODEL_30ebd351ba2a45e8ad21adf6382a306d",
       "IPY_MODEL_c121b8cb8f794fd0890275e8b9016743"
      ],
      "layout": "IPY_MODEL_2390139f26ca4e3796de697185e2e9f1"
     }
    },
    "bc4d7e1e366543dbbc4d52bab43ab7f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c121b8cb8f794fd0890275e8b9016743": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b0453d5a80e54e3e8e207c4f42bc7d08",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_bc4d7e1e366543dbbc4d52bab43ab7f6",
      "value": "â€‡3/3â€‡[01:09&lt;00:00,â€‡22.62s/it]"
     }
    },
    "edab26fd431b4997b7a1bf7d85716387": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
