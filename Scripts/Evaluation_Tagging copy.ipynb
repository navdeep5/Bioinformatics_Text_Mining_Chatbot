{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RB9wEEWpe1Jt",
        "outputId": "9818ef97-55d3-4a74-fca6-56f2a94026e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jaro-winkler in c:\\users\\navde\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.3)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\navde\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\navde\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.23.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\navde\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.8.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\navde\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\navde\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: torcheval in c:\\users\\navde\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.7)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\navde\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torcheval) (4.9.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install jaro-winkler\n",
        "!pip install scikit-learn\n",
        "!pip install torcheval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torcheval.metrics.functional import binary_f1_score, binary_precision, binary_recall\n",
        "import json\n",
        "from jaro import jaro_winkler_metric\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def to_binary(preds: list[tuple], gt: list[tuple]) -> tuple[list[int], list[int]]:\n",
        "    gt = set(gt)\n",
        "    output = []\n",
        "    target = []\n",
        "    for pred in preds:\n",
        "        if pred in gt:\n",
        "            # real triplet (true positive)\n",
        "            gt.remove(pred)\n",
        "            target.append(1)\n",
        "        else:\n",
        "            # fake triplet (false positive)\n",
        "            target.append(0)\n",
        "\n",
        "        # positive\n",
        "        output.append(1)\n",
        "\n",
        "    # unextracted triplets (false negatives)\n",
        "    target.extend([1 for item in gt])\n",
        "    output.extend([0 for item in gt])\n",
        "\n",
        "    return output, target\n",
        "\n",
        "def f1(preds: list[list[tuple[str]]], gt: list[list[tuple[str]]]) -> list:\n",
        "    # N = len(gt)\n",
        "    N = min(len(gt), len(preds))\n",
        "    scores = []\n",
        "\n",
        "    for i in range(N):\n",
        "        # print(\"preds\", preds[i])\n",
        "        # print(\"gt\", gt[i])\n",
        "        output, target = to_binary(preds[i], gt[i])\n",
        "        output, target = torch.tensor(output), torch.tensor(target)\n",
        "        # print(\"out\", output)\n",
        "        # print(\"target\", target)\n",
        "        scores.append(float(binary_f1_score(output, target)))\n",
        "\n",
        "    return scores\n",
        "\n",
        "def precision(preds: list[list[tuple[str]]], gt: list[list[tuple[str]]]) -> list:\n",
        "    # N = len(gt)\n",
        "    N = min(len(gt), len(preds))\n",
        "    scores = []\n",
        "\n",
        "    for i in range(N):\n",
        "        output, target = to_binary(preds[i], gt[i])\n",
        "        output, target = torch.tensor(output), torch.tensor(target)\n",
        "        scores.append(float(binary_precision(output, target)))\n",
        "\n",
        "    return scores\n",
        "\n",
        "def recall(preds: list[list[tuple[str]]], gt: list[list[tuple[str]]]) -> list:\n",
        "    # N = len(gt)\n",
        "    N = min(len(gt), len(preds))\n",
        "    scores = []\n",
        "\n",
        "    for i in range(N):\n",
        "        output, target = to_binary(preds[i], gt[i])\n",
        "        output, target = torch.tensor(output), torch.tensor(target)\n",
        "        scores.append(float(binary_recall(output, target)))\n",
        "\n",
        "    return scores\n",
        "\n",
        "def jaro_winkler(preds: list[list[tuple[str]]], gt: list[list[tuple[str]]]) -> list:\n",
        "    # N = len(gt)\n",
        "    N = min(len(gt), len(preds))\n",
        "    scores = []\n",
        "\n",
        "    for i in range(N):\n",
        "        output = json.dumps(preds[i])\n",
        "        target = json.dumps(gt[i])\n",
        "        scores.append(jaro_winkler_metric(output, target))\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kkZdsSoFrVK_",
        "outputId": "cfae1095-e860-4749-f122-dc7db0d96ebb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "50it [00:00, 120.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score:\n",
            "  Average: 0.6889447236180904\n",
            "  Minimum: 0.0\n",
            "  Maximum: 1.0\n",
            "  Standard Deviation: 0.4358073337157155\n",
            "Precision:\n",
            "  Average: 0.6889447236180904\n",
            "  Minimum: 0.0\n",
            "  Maximum: 1.0\n",
            "  Standard Deviation: 0.4358073337157155\n",
            "Recall:\n",
            "  Average: 0.6889447236180904\n",
            "  Minimum: 0.0\n",
            "  Maximum: 1.0\n",
            "  Standard Deviation: 0.4358073337157155\n",
            "Jaro-Winkler Score:\n",
            "  Average: 0.8726224245036122\n",
            "  Minimum: 0.41516582153057063\n",
            "  Maximum: 1.0\n",
            "  Standard Deviation: 0.1832049838217127\n",
            "Successful Outputs: 50\n",
            "Failed Outputs: 0\n",
            "Total Outputs: 50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Tagger/results_of_tagger_bias_2.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[25], line 94\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Outputs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_outputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Write results to a file\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTagger/results_of_tagger_bias_2.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, score_list \u001b[38;5;129;01min\u001b[39;00m [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1 Score\u001b[39m\u001b[38;5;124m\"\u001b[39m, f1_score_list), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision\u001b[39m\u001b[38;5;124m\"\u001b[39m, precision_list), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m\"\u001b[39m, recall_list), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJaro-Winkler Score\u001b[39m\u001b[38;5;124m\"\u001b[39m, jw_score_list)]:\n\u001b[0;32m     96\u001b[0m         avg_score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(score_list)\n",
            "File \u001b[1;32mc:\\Users\\navde\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Tagger/results_of_tagger_bias_2.txt'"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import statistics\n",
        "from tqdm import tqdm\n",
        "from json import JSONDecodeError\n",
        "import re\n",
        "\n",
        "def extract_tags(text):\n",
        "    # Regular expression to match substrings enclosed within <>\n",
        "    pattern = r'<([^<]*?)>(.*?)<\\/\\1>'\n",
        "\n",
        "    # Find all tagged substrings\n",
        "    tagged_substrings = re.findall(pattern, text)\n",
        "\n",
        "    # Return each tag and content separately\n",
        "    tagged_lists = [[tag, content] for tag, content in tagged_substrings]\n",
        "\n",
        "    return tagged_lists\n",
        "\n",
        "# def extract_tags(text):\n",
        "#     # Regular expression pattern to match substrings enclosed within <>\n",
        "#     pattern = r'<[^<]*?>.*?<\\/[^<]*?>'\n",
        "\n",
        "#     # Find all tagged substrings\n",
        "#     tagged_substrings = re.findall(pattern, text)\n",
        "\n",
        "#     # Split each tagged substring into individual tags and content\n",
        "#     tagged_lists = []\n",
        "#     for substring in tagged_substrings:\n",
        "#         # Extract individual tags and content\n",
        "#         tags_and_content = re.findall(r'<([^>]+)>(.*?)<\\/\\1>', substring)\n",
        "#         tagged_lists.extend(tags_and_content)\n",
        "\n",
        "#     return tagged_lists\n",
        "\n",
        "\n",
        "\n",
        "successful_output = 0\n",
        "\n",
        "# Create lists\n",
        "f1_score_list = []\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "jw_score_list = []\n",
        "successful_output = 0\n",
        "failed_output = 0\n",
        "\n",
        "test_set_path = 'C:/Users/navde/Desktop/Bioinformatics_401_Project/Data/Tagging_50_Test.jsonl'\n",
        "truth_file_path = 'C:/Users/navde/Desktop/Bioinformatics_401_Project/Data/Tagging_Ground_Truth_Bias.jsonl'\n",
        "\n",
        "with open(test_set_path, 'r',encoding=\"utf-8\") as infile, open(truth_file_path, \"r\",encoding=\"utf-8\") as outfile:\n",
        "    for line, gt_line in tqdm(zip(infile, outfile)):\n",
        "        successful_output += 1\n",
        "        data = json.loads(line)\n",
        "        datagt = json.loads(gt_line)\n",
        "        abstract = data['input']\n",
        "        ground_truth = datagt['input']\n",
        "\n",
        "        # print(extract_tags(abstract))\n",
        "        # print(extract_tags(ground_truth))\n",
        "\n",
        "        # Get scores\n",
        "        f1_score = f1(extract_tags(abstract), extract_tags(ground_truth))\n",
        "        precision_value = precision(extract_tags(abstract), extract_tags(ground_truth))\n",
        "        recall_val = recall(extract_tags(abstract), extract_tags(ground_truth))\n",
        "        jw_score = jaro_winkler(extract_tags(abstract), extract_tags(ground_truth))\n",
        "\n",
        "        #Increment variables\n",
        "        f1_score_list.extend(f1_score)\n",
        "        precision_list.extend(precision_value)\n",
        "        recall_list.extend(recall_val)\n",
        "        jw_score_list.extend(jw_score)\n",
        "\n",
        "# Calculate statistics for each list\n",
        "for name, score_list in [(\"F1 Score\", f1_score_list), (\"Precision\", precision_list), (\"Recall\", recall_list), (\"Jaro-Winkler Score\", jw_score_list)]:\n",
        "    avg_score = np.mean(score_list)\n",
        "    min_score = np.min(score_list)\n",
        "    max_score = np.max(score_list) \n",
        "    std_dev = np.std(score_list) # Std. dev. requires at least 2 data points\n",
        "\n",
        "    # Print statistics\n",
        "    print(f\"{name}:\")\n",
        "    print(f\"  Average: {avg_score}\")\n",
        "    print(f\"  Minimum: {min_score}\")\n",
        "    print(f\"  Maximum: {max_score}\")\n",
        "    print(f\"  Standard Deviation: {std_dev}\")\n",
        "\n",
        "# Print other information\n",
        "total_outputs = successful_output + failed_output\n",
        "print(f\"Successful Outputs: {successful_output}\")\n",
        "print(f\"Failed Outputs: {failed_output}\")\n",
        "print(f\"Total Outputs: {total_outputs}\")\n",
        "\n",
        "# Write results to a file\n",
        "with open(\"Tagger/results_of_tagger_bias_2.txt\", \"w\") as f:\n",
        "    for name, score_list in [(\"F1 Score\", f1_score_list), (\"Precision\", precision_list), (\"Recall\", recall_list), (\"Jaro-Winkler Score\", jw_score_list)]:\n",
        "        avg_score = np.mean(score_list)\n",
        "        min_score = np.min(score_list)\n",
        "        max_score = np.max(score_list) \n",
        "        std_dev = np.std(score_list) #if len(score_list) > 1 else 0  # Std. dev. requires at least 2 data points\n",
        "\n",
        "        # Write statistics to file\n",
        "        f.write(f\"{name}:\\n\")\n",
        "        f.write(f\"  Average: {avg_score}\\n\")\n",
        "        f.write(f\"  Minimum: {min_score}\\n\")\n",
        "        f.write(f\"  Maximum: {max_score}\\n\")\n",
        "        f.write(f\"  Standard Deviation: {std_dev}\\n\")\n",
        "\n",
        "    # Write other information\n",
        "    total_outputs = successful_output + failed_output\n",
        "    f.write(f\"Successful Outputs: {successful_output}\\n\")\n",
        "    f.write(f\"Failed Outputs: {failed_output}\\n\")\n",
        "    f.write(f\"Total Outputs: {total_outputs}\\n\")\n",
        "\n",
        "print(\"Results have been written to results_of_tagger.txt.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
